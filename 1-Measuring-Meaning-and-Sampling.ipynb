{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Measuring Meaning & Sampling\n",
    "This week, we begin by \"begging, borrowing and stealing\" text from several\n",
    "contexts of human communication (e.g., PDFs, HTML, Word) and preparing it for\n",
    "machines to \"read\" and analyze so that we can begin to build our sample. This notebook outlines scraping text from the web, PDF and Word documents. Then we detail \"spidering\" or walking\n",
    "through hyperlinks to build samples of online content, and using APIs,\n",
    "Application Programming Interfaces, provided by webservices to access their\n",
    "content. Along the way, we will use regular expressions, outlined in the\n",
    "reading, to remove unwanted formatting and ornamentation. Next, we discuss\n",
    "various text encodings, filtering and data structures in which text can be\n",
    "placed for analysis. Finally, we ask you to begin building a corpus for preliminary analysis and articulate what your sample represents in context of your final project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a python package just for this course: lucem_illud. If you haven't installed this package, you should run the following code first. You don't need to install the package later; all you need to do is just to import the package with: import lucem_illud. For your final projects, you may find it useful to [read the lucem_illud source code](https://github.com/UChicago-Computational-Content-Analysis/lucem_illud/tree/main/lucem_illud) and modify your code for your own interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.394848Z",
     "start_time": "2024-01-11T05:20:51.989046Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "#installing lucem_illud package\n",
    "#lucem_illud is a Latin phrase meaning \"that light\", the insight we can discover in text data!\n",
    "#If you get an error like \"Access is denied\", try running the `pip` command on the command line as an administrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not familiar with jupyter notebook, you may wonder what the exclamation mark(!) at the beginning of the command does (or even what pip means). The exclamation mark enables us to execute Terminal commands in the notebook cells (e.g., run `!ls` to display files in the current folder).\n",
    "\n",
    "There is also a special download required by the `lucem_illud` module in the module `spacy`. You will see this 'en' module later, but you should probably run the following 2 lines of code so you can import `lucem_illud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.399707Z",
     "start_time": "2024-01-11T05:20:51.996681Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.407287Z",
     "start_time": "2024-01-11T05:20:52.001602Z"
    }
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "import lucem_illud #pip install git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import requests #for http requests\n",
    "import bs4 #called `beautifulsoup4`, an html parser\n",
    "import pandas #gives us DataFrames\n",
    "import docx #reading MS doc files, install as `python-docx`\n",
    "\n",
    "#Stuff for pdfs\n",
    "#Install as `pdfminer2`\n",
    "import pdfminer.pdfinterp\n",
    "import pdfminer.converter\n",
    "import pdfminer.layout\n",
    "import pdfminer.pdfpage\n",
    "\n",
    "#These come with Python\n",
    "import re #for regexs\n",
    "import urllib.parse #For joining urls\n",
    "import io #for making http requests look like files\n",
    "import json #For Tumblr API responses\n",
    "import os.path #For checking if files exist\n",
    "import os #For making directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also be working on the following files/urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.409627Z",
     "start_time": "2024-01-11T05:20:52.004834Z"
    }
   },
   "outputs": [],
   "source": [
    "wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "example_text_file = 'sometextfile.txt'\n",
    "information_extraction_pdf = 'https://github.com/Computational-Content-Analysis-2018/Data-Files/raw/master/1-intro/Content%20Analysis%2018.pdf'\n",
    "example_docx = 'https://github.com/Computational-Content-Analysis-2018/Data-Files/raw/master/1-intro/macs6000_connecting_to_midway.docx'\n",
    "example_docx_save = 'example.docx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "Before we can start analyzing content we need to obtain it. Sometimes it will be\n",
    "provided to us from a pre-curated text archive, but sometimes we will need to\n",
    "download it. As a starting example we will attempt to download the wikipedia\n",
    "page on content analysis. The page is located at [https://en.wikipedia.org/wiki/\n",
    "Content_analysis](https://en.wikipedia.org/wiki/Content_analysis) so lets start\n",
    "with that.\n",
    "\n",
    "We can do this by making an HTTP GET request to that url, a GET request is\n",
    "simply a request to the server to provide the contents given by some url. The\n",
    "other request we will be using in this class is called a POST request and\n",
    "requests the server to take some content we provide. While the Python standard\n",
    "library does have the ability do make GET requests we will be using the\n",
    "[_requests_](http://docs.python-requests.org/en/master/) package as it is _'the\n",
    "only Non-GMO HTTP library for Python'_...also it provides a nicer interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.520276Z",
     "start_time": "2024-01-11T05:20:52.017225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Response [200]>"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "requests.get(wikipedia_content_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'Response [200]'` means the server responded with what we asked for. If you get\n",
    "another number (e.g. 404) it likely means there was some kind of error, these\n",
    "codes are called HTTP response codes and a list of them can be found\n",
    "[here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). The response\n",
    "object contains all the data the server sent including the website's contents\n",
    "and the HTTP header. We are interested in the contents which we can access with\n",
    "the `.text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.637710Z",
     "start_time": "2024-01-11T05:20:52.338122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-zebra-design-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-toc-available\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\">\n",
      "<title>Content analysis - Wikipedia</title>\n",
      "<script>(function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-cli\n"
     ]
    }
   ],
   "source": [
    "wikiContentRequest = requests.get(wikipedia_content_analysis)\n",
    "print(wikiContentRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we were looking for, because it is the start of the HTML that\n",
    "makes up the website. This is HTML and is meant to be read by computers. Luckily\n",
    "we have a computer to parse it for us. To do the parsing we will use [_Beautiful\n",
    "Soup_](https://www.crummy.com/software/BeautifulSoup/) which is a better parser\n",
    "than the one in the standard library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we proceed to Beautiful Soup, a digression about Python syntax, especially about objects and functions.\n",
    "For those who are not familiar with the syntax of python (or, if you're familiar with R programming), you might wonder what requests.get or wikiContentRequest.text mean. To understand this, you need to first understand what objects are. You may have heard that Python is an object oriented programming language (unlike the procedure oriented programming language, an example of which is R). Object is a set of variables (or, data) and functions into which you pass your data. So, in object oriented programming languages, like python, variables and functions are bunleded into objects.\n",
    "\n",
    "For example, let's look at wikiContentRequest. We use dir() function, which returns the list of attributes and functions of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.640906Z",
     "start_time": "2024-01-11T05:20:52.638481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['__attrs__',\n '__bool__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__enter__',\n '__eq__',\n '__exit__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__nonzero__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_content',\n '_content_consumed',\n '_next',\n 'apparent_encoding',\n 'close',\n 'connection',\n 'content',\n 'cookies',\n 'elapsed',\n 'encoding',\n 'headers',\n 'history',\n 'is_permanent_redirect',\n 'is_redirect',\n 'iter_content',\n 'iter_lines',\n 'json',\n 'links',\n 'next',\n 'ok',\n 'raise_for_status',\n 'raw',\n 'reason',\n 'request',\n 'status_code',\n 'text',\n 'url']"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dir(wikiContentRequest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 'text' here. We used 'wikiContentRequest.text' to access 'text.' In other words, we use .(dot notation) to access functions from objects. wikiContentRequest has a set of functions, as shown above, and we used 'wikiContentRequest.text' to access one of them. By the way, dot notations do not necessarily refer to functions--it refers to anything that the entity contains. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the next step: BeautifulSoup, a Python library which extracts data from HTML and XML, and transforms HTML files into Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.846355Z",
     "start_time": "2024-01-11T05:20:52.843681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Content analysis - Wikipedia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jump to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\t\tNavigation\n",
      "\t\n",
      "\n",
      "\n",
      "Main pageContentsCurrent eventsRandom arti\n"
     ]
    }
   ],
   "source": [
    "wikiContentSoup = bs4.BeautifulSoup(wikiContentRequest.text, 'html.parser')\n",
    "print(wikiContentSoup.text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better but there's still random whitespace and we have more than just\n",
    "the text of the article. This is because what we requested is the whole webpage,\n",
    "not just the text for the article.\n",
    "\n",
    "We want to extract only the text we care about, and in order to do this we will\n",
    "need to inspect the html. One way to do this is simply to go to the website with\n",
    "a browser and use its inspection or view source tool. If javascript or other\n",
    "dynamic loading occurs on the page, however, it is likely that what Python\n",
    "receives is not what you will see, so we will need to inspect what Python\n",
    "receives. To do this we can save the html `requests` obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.850219Z",
     "start_time": "2024-01-11T05:20:52.846001Z"
    }
   },
   "outputs": [],
   "source": [
    "#content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "\n",
    "with open(content_analysis_save, mode='w', encoding='utf-8') as f:\n",
    "    f.write(wikiContentRequest.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open() is a function which literally opens and returns the file. This function has multiple modes, and, here, we used mode as 'w', which means: open a file for writing. And then, we use 'write' function to write on the empty file (content_analysis_save) that we created using open(content_analysis_save, mode='w', encoding='utf-8').} What did we write on this file? The text we got from wikiContentRequest.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's open the file (`wikipedia_content_analysis.html`) we just created with\n",
    "a web browser. It should look sort of like the original but without the images\n",
    "and formatting.\n",
    "\n",
    "As there is very little standardization on structuring webpages, figuring out\n",
    "how best to extract what you want is an art. Looking at this page it looks like\n",
    "all the main textual content is inside `<p>`(paragraph) tags within the `<body>`\n",
    "tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.852706Z",
     "start_time": "2024-01-11T05:20:52.850496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content analysis is the study of documents and communication artifacts, which might be texts of various formats, pictures, audio or video. Social scientists use content analysis to examine patterns in communication in a replicable and systematic manner.[1] One of the key advantages of using content analysis to analyse social phenomena is their non-invasive nature, in contrast to simulating social experiences or collecting survey answers.\n",
      "\n",
      "Practices and philosophies of content analysis vary between academic disciplines. They all involve systematic reading or observation of texts or artifacts which are assigned labels (sometimes called codes) to indicate the presence of interesting, meaningful pieces of content.[2][3] By systematically labeling the content of a set of texts, researchers can analyse patterns of content quantitatively using statistical methods, or use qualitative methods to analyse meanings of content within texts.\n",
      "\n",
      "Computers are increasingly used in content analysis to automate the labeling (or coding) of documents. Simple computational techniques can provide descriptive data such as word frequencies and document lengths. Machine learning classifiers can greatly increase the number of texts that can be labeled, but the scientific utility of doing so is a matter of debate. Further, numerous computer-aided text analysis (CATA) computer programs are available that analyze text for pre-determined linguistic, semantic, and psychological characteristics.[4]\n"
     ]
    }
   ],
   "source": [
    "contentPTags = wikiContentSoup.body.findAll('p')\n",
    "for pTag in contentPTags[:3]:\n",
    "    print(pTag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another excursion for those who are not familiar with programming: for loop. For loop is used to iterate over a sequence. \"ContentPTags\" contains multiple paragraphs, each of which starts and ends with `<p>`. What the \"for pTag in contentPtags[:3]\" does here is: find each paragraph in contentPTags, which, here, we limited to the first three using contentPtags[:3], and then print each paragraph. So, we have three paragraphs. By the way, you can insert `<p>` in juputer notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the text from the page, split up by paragraph. If we wanted to\n",
    "get the section headers or references as well it would require a bit more work,\n",
    "but is doable.\n",
    "\n",
    "There is one more thing we might want to do before sending this text to be\n",
    "processed, remove the references indicators (`[2]`, `[3]` , etc). To do this we\n",
    "can use a short regular expression (regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.856587Z",
     "start_time": "2024-01-11T05:20:52.854205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       paragraph-text\n",
      "0   Content analysis is the study of documents and...\n",
      "1   Practices and philosophies of content analysis...\n",
      "2   Computers are increasingly used in content ana...\n",
      "3   Content analysis is best understood as a broad...\n",
      "4   The simplest and most objective form of conten...\n",
      "5   A further step in analysis is the distinction ...\n",
      "6   Quantitative content analysis highlights frequ...\n",
      "7   Siegfried Kracauer provides a critique of quan...\n",
      "8   The data collection instrument used in content...\n",
      "9   According to current standards of good scienti...\n",
      "10  Furthermore, the Database of Variables for Con...\n",
      "11  With the rise of common computing facilities l...\n",
      "12  By having contents of communication available ...\n",
      "13  Computer-assisted analysis can help with large...\n",
      "14  Robert Weber notes: \"To make valid inferences ...\n",
      "15  According to today's reporting standards, quan...\n",
      "16  There are five types of texts in content analy...\n",
      "17  Content analysis is research using the categor...\n",
      "18  Over the years, content analysis has been appl...\n",
      "19  In recent times, particularly with the advent ...\n",
      "20  Quantitative content analysis has enjoyed a re...\n",
      "21  Content analysis can also be described as stud...\n",
      "22  Manifest content is readily understandable at ...\n",
      "23  Holsti groups fifteen uses of content analysis...\n",
      "24  He also places these uses into the context of ...\n",
      "25  The following table shows fifteen uses of cont...\n",
      "26  As a counterpoint, there are limits to the sco...\n",
      "27  The process of the initial coding scheme or ap...\n",
      "28  With either approach above, immersing oneself ...\n"
     ]
    }
   ],
   "source": [
    "contentParagraphs = []\n",
    "for pTag in contentPTags:\n",
    "    #strings starting with r are raw so their \\'s are not modifier characters\n",
    "    #If we didn't start with r the string would be: '\\\\[\\\\d+\\\\]'\n",
    "    contentParagraphs.append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "\n",
    "#convert to a DataFrame\n",
    "contentParagraphsDF = pandas.DataFrame({'paragraph-text' : contentParagraphs})\n",
    "print(contentParagraphsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we learned how to do for loop, you might get what we did here: using contentParagraphs = [], we made an empty list; and then, for each paragraph in contentPTags, we substituted every [\\d+\\] with '', i.e., removed every [\\d+\\], and then appended each paragraph (now without [\\d+\\]) to the empty list. As we can see, we have a dataframe, each row of which is each paragraph of contentPTags, without reference indicators. \n",
    "\n",
    "By the way, what does [\\d+\\] mean? If you are not familiar with regex, it is a way of specifying searches in text.\n",
    "A regex engine takes in the search pattern, in the above case `'\\[\\d+\\]'` and\n",
    "some string, the paragraph texts. Then it reads the input string one character\n",
    "at a time checking if it matches the search. Here the regex `'\\d'` matches\n",
    "number characters (while `'\\['` and `'\\]'` capture the braces on either side)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a `DataFrame` containing all relevant text from the page ready to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.858839Z",
     "start_time": "2024-01-11T05:20:52.856979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<re.Match object; span=(36, 37), match='2'>"
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findNumber = r'\\d'\n",
    "regexResults = re.search(findNumber, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "regexResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python the regex package (`re`) usually returns `Match` objects (you can have\n",
    "multiple pattern hits in a a single `Match`), to get the string that matched our\n",
    "pattern we can use the `.group()` method, and as we want the first one we will\n",
    "ask for the 0'th group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.860840Z",
     "start_time": "2024-01-11T05:20:52.859433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us the first number, if we wanted the whole block of numbers we can\n",
    "add a wildcard `'+'` which requests 1 or more instances of the preceding\n",
    "character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:52.865317Z",
     "start_time": "2024-01-11T05:20:52.862923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2134567890\n"
     ]
    }
   ],
   "source": [
    "findNumbers = r'\\d+'\n",
    "regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the whole block of numbers, there are a huge number of special\n",
    "characters in regex, for the full description of Python's implementation look at\n",
    "the [re docs](https://docs.python.org/3/library/re.html) there is also a short\n",
    "[tutorial](https://docs.python.org/3/howto/regex.html#regex-howto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"white\">Exercise 1</font>\n",
    "<font color=\"white\">Construct cells immediately below this that describe and download webcontent relating to your anticipated final project. Use beautiful soup and at least five regular expressions to extract relevant, nontrivial *chunks* of that content (e.g., cleaned sentences, paragraphs, etc.) to a pandas `Dataframe`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "             news_title       news_sub_title layout_num layout_name  \\\n0  “一江一河”滨水空间打造更多有温度的服务  带“汪汪队”逛店 与“毛孩子”友好同行          5        上海新闻   \n\n         date  year month day     authors  \\\n0  2024-01-10  2024    01  10  [杨玉红, 钱文婷]   \n\n                                             article photographers  \n0  周末带“毛孩子”去哪里撒欢，成为申城不少爱宠人士交流的热门话题。家门口的滨水公共空间成为不少...     [周馨, 钱文婷]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_title</th>\n      <th>news_sub_title</th>\n      <th>layout_num</th>\n      <th>layout_name</th>\n      <th>date</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>authors</th>\n      <th>article</th>\n      <th>photographers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>“一江一河”滨水空间打造更多有温度的服务</td>\n      <td>带“汪汪队”逛店 与“毛孩子”友好同行</td>\n      <td>5</td>\n      <td>上海新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[杨玉红, 钱文婷]</td>\n      <td>周末带“毛孩子”去哪里撒欢，成为申城不少爱宠人士交流的热门话题。家门口的滨水公共空间成为不少...</td>\n      <td>[周馨, 钱文婷]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_web = 'https://paper.xinmin.cn/html/xmwb/2024-01-10/5/176996.html'\n",
    "news_content_analysis = 'news_content_analysis.html'\n",
    "news_web_request = requests.get(news_web)\n",
    "news_web_request.encoding = 'utf-8'\n",
    "news_content_soup = bs4.BeautifulSoup(news_web_request.text, 'html.parser')\n",
    "news_title = news_content_soup.find_all('span', class_ = 'dzb-special-title-box')\n",
    "news_title_lst = [span.get_text() for span in news_title]\n",
    "news_sub_title = news_content_soup.find_all('h2', class_ = 'dzb-title-box')\n",
    "news_sub_title_lst = [span.get_text() for span in news_sub_title]\n",
    "layout_info = news_content_soup.find_all('span', class_ = 'dzb-banmian-title')\n",
    "layout_text = [span.get_text() for span in layout_info]\n",
    "layout_num_pattern = re.compile(r'第(\\d+)版')\n",
    "layout_name_pattern = re.compile(r'：(.*)')\n",
    "layout_num = [re.search(layout_num_pattern, layout_text[0]).group(1)]\n",
    "layout_name = [re.search(layout_name_pattern, layout_text[0]).group(1)]\n",
    "date_info = news_content_soup.find_all('span', class_ = 'dzb-banmian-date')\n",
    "date_text = [span.get_text() for span in date_info]\n",
    "date_pattern = re.compile(r'((\\d{4})-(\\d{2})-(\\d{2}))')\n",
    "date_match = re.search(date_pattern, date_text[0])\n",
    "date = [date_match.group(1)]\n",
    "year = [date_match.group(2)]\n",
    "month = [date_match.group(3)]\n",
    "day = [date_match.group(4)]\n",
    "report_info = news_content_soup.find_all('p', class_ = 'dzb-desc-box')\n",
    "report_text = report_info[0].get_text(separator = '\\n')\n",
    "authors_article = report_text.split('\\n', 1)\n",
    "authors_text = authors_article[0]\n",
    "report_text = authors_article[1].strip()\n",
    "authors_pattern = r'本报记者\\s+([\\u4e00-\\u9fa5\\s]+)'\n",
    "authors_match = re.search(authors_pattern, authors_text)\n",
    "authors = [authors_match.group(1).split()]\n",
    "article = [report_text]\n",
    "photograph_info = news_content_soup.find_all('p', style=\"padding: 3px;white-space: pre-line;word-break: normal;\")\n",
    "photograph_info_lst = [p.get_text() for p in photograph_info]\n",
    "photographer_pattern = r'本报记者\\s+([\\u4e00-\\u9fa5]+)\\s+摄'\n",
    "photographer_lst = []\n",
    "for text in photograph_info_lst:\n",
    "    photographer_match = re.search(photographer_pattern, text)\n",
    "    if photographer_match.group(1) not in photographer_lst:\n",
    "        photographer_lst.append(photographer_match.group(1))\n",
    "photographer_lst = [photographer_lst]\n",
    "news_info_df = pandas.DataFrame({\n",
    "    'news_title' : news_title_lst,\n",
    "    'news_sub_title' : news_sub_title_lst,\n",
    "    'layout_num' : layout_num,\n",
    "    'layout_name' : layout_name,\n",
    "    'date' : date,\n",
    "    'year' : year,\n",
    "    'month' : month,\n",
    "    'day' : day,\n",
    "    'authors' : authors,\n",
    "    'article' : article,\n",
    "    'photographers' : photographer_lst})\n",
    "news_info_df\n",
    "#with open(news_content_analysis, mode='w', encoding='utf-8') as f:\n",
    "#    f.write(news_web_request.text)\n",
    "#news_content_p_tags = news_content_soup.body.findAll('p')\n",
    "#for p_tag in news_content_p_tags:\n",
    "#    print(p_tag.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:54.112782Z",
     "start_time": "2024-01-11T05:20:52.869198Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T05:01:15.776538Z",
     "start_time": "2024-01-10T05:01:14.659437Z"
    }
   },
   "source": [
    "\n",
    "# Spidering\n",
    "\n",
    "What if we want to to get a bunch of different pages from wikipedia. We would\n",
    "need to get the url for each of the pages we want. Typically, we want pages that\n",
    "are linked to by other pages and so we will need to parse pages and identify the\n",
    "links. Right now we will be retrieving all links in the body of the content\n",
    "analysis page.\n",
    "\n",
    "To do this we will need to find all the `<a>` (anchor) tags with `href`s\n",
    "(hyperlink references) inside of `<p>` tags. `href` can have many\n",
    "[different](http://stackoverflow.com/questions/4855168/what-is-href-and-why-is-\n",
    "it-used) [forms](https://en.wikipedia.org/wiki/Hyperlink#Hyperlinks_in_HTML) so\n",
    "dealing with them can be tricky, but generally, you will want to extract\n",
    "absolute or relative links. An absolute link is one you can follow without\n",
    "modification, while a relative link requires a base url that you will then\n",
    "append. Wikipedia uses relative urls for its internal links: below is an example\n",
    "for dealing with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:54.118084Z",
     "start_time": "2024-01-11T05:20:54.115369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://en.wikipedia.org/wiki/Document', 0, 'documents'), ('https://en.wikipedia.org/wiki/Text_(literary_theory)', 1, 'texts'), ('https://en.wikipedia.org/wiki/Coding_(social_sciences)', 1, 'assigned labels (sometimes called codes)'), ('https://en.wikipedia.org/wiki/Semantics', 1, 'meaningful'), ('https://en.wikipedia.org/wiki/Text_(literary_theory)', 1, 'texts'), ('https://en.wikipedia.org/wiki/Quantitative_research', 1, 'quantitatively'), ('https://en.wikipedia.org/wiki/Statistics', 1, 'statistical methods'), ('https://en.wikipedia.org/wiki/Qualitative_research', 1, 'qualitative'), ('https://en.wikipedia.org/wiki/Text_(literary_theory)', 1, 'texts'), ('https://en.wikipedia.org/wiki/Machine_learning', 2, 'Machine learning')]\n"
     ]
    }
   ],
   "source": [
    "#wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "otherPAgeURLS = []\n",
    "#We also want to know where the links come from so we also will get:\n",
    "#the paragraph number\n",
    "#the word the link is in\n",
    "for paragraphNum, pTag in enumerate(contentPTags):\n",
    "    #we only want hrefs that link to wiki pages\n",
    "    tagLinks = pTag.findAll('a', href=re.compile('/wiki/'), class_=False)\n",
    "    for aTag in tagLinks:\n",
    "        #We need to extract the url from the <a> tag\n",
    "        relurl = aTag.get('href')\n",
    "        linkText = aTag.text\n",
    "        #wikipedia_base_url is the base we can use the urllib joining function to merge them\n",
    "        #Giving a nice structured tupe like this means we can use tuple expansion later\n",
    "        otherPAgeURLS.append((\n",
    "            urllib.parse.urljoin(wikipedia_base_url, relurl),\n",
    "            paragraphNum,\n",
    "            linkText,\n",
    "        ))\n",
    "print(otherPAgeURLS[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:54.123373Z",
     "start_time": "2024-01-11T05:20:54.120721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p><b>Content analysis</b> is the study of <a href=\"/wiki/Document\" title=\"Document\">documents</a> and communication artifacts, which might be texts of various formats, pictures, audio or video. Social scientists use content analysis to examine patterns in communication in a replicable and systematic manner.<sup class=\"reference\" id=\"cite_ref-1\"><a href=\"#cite_note-1\">[1]</a></sup> One of the key advantages of using content analysis to analyse social phenomena is their non-invasive nature, in contrast to simulating social experiences or collecting survey answers.\n",
      "</p>, <p>Practices and philosophies of content analysis vary between academic disciplines. They all involve systematic reading or observation of <a href=\"/wiki/Text_(literary_theory)\" title=\"Text (literary theory)\">texts</a> or artifacts which are <a href=\"/wiki/Coding_(social_sciences)\" title=\"Coding (social sciences)\">assigned labels (sometimes called codes)</a> to indicate the presence of interesting, <a href=\"/wiki/Semantics\" title=\"Semantics\">meaningful</a> pieces of content.<sup class=\"reference\" id=\"cite_ref-2\"><a href=\"#cite_note-2\">[2]</a></sup><sup class=\"reference\" id=\"cite_ref-Tipaldo_2014_42_3-0\"><a href=\"#cite_note-Tipaldo_2014_42-3\">[3]</a></sup> By systematically labeling the content of a set of <a href=\"/wiki/Text_(literary_theory)\" title=\"Text (literary theory)\">texts</a>, researchers can analyse patterns of content <a href=\"/wiki/Quantitative_research\" title=\"Quantitative research\">quantitatively</a> using <a href=\"/wiki/Statistics\" title=\"Statistics\">statistical methods</a>, or use <a href=\"/wiki/Qualitative_research\" title=\"Qualitative research\">qualitative</a> methods to analyse meanings of content within <a href=\"/wiki/Text_(literary_theory)\" title=\"Text (literary theory)\">texts</a>.\n",
      "</p>, <p>Computers are increasingly used in content analysis to automate the labeling (or coding) of documents. Simple computational techniques can provide descriptive data such as word frequencies and document lengths. <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a> classifiers can greatly increase the number of texts that can be labeled, but the scientific utility of doing so is a matter of debate. Further, numerous computer-aided text analysis (CATA) computer programs are available that analyze text for pre-determined linguistic, semantic, and psychological characteristics.<sup class=\"reference\" id=\"cite_ref-Neuendorf2016_4-0\"><a href=\"#cite_note-Neuendorf2016-4\">[4]</a></sup>\n",
      "</p>, <p>Content analysis is best understood as a broad family of techniques. Effective researchers choose techniques that best help them answer their substantive questions. That said, according to <a href=\"/wiki/Klaus_Krippendorff\" title=\"Klaus Krippendorff\">Klaus Krippendorff</a>, six questions must be addressed in every content analysis:<sup class=\"reference\" id=\"cite_ref-Krippendorff2004_5-0\"><a href=\"#cite_note-Krippendorff2004-5\">[5]</a></sup>\n",
      "</p>, <p>The simplest and most objective form of content analysis considers unambiguous characteristics of the text such as <a class=\"mw-redirect\" href=\"/wiki/Word_frequencies\" title=\"Word frequencies\">word frequencies</a>, the page area taken by a newspaper column, or the duration of a <a href=\"/wiki/Radio\" title=\"Radio\">radio</a> or <a href=\"/wiki/Television\" title=\"Television\">television</a> program. Analysis of simple word frequencies is limited because the meaning of a word depends on surrounding text.  <a href=\"/wiki/Key_Word_in_Context\" title=\"Key Word in Context\">Key Word In Context</a> (KWIC) routines address this by placing words in their textual context. This helps resolve ambiguities such as those introduced by <a href=\"/wiki/Synonym\" title=\"Synonym\">synonyms</a> and <a href=\"/wiki/Homonym\" title=\"Homonym\">homonyms</a>.\n",
      "</p>, <p>A further step in analysis is the distinction between dictionary-based (quantitative) approaches and qualitative approaches. Dictionary-based approaches set up a list of categories derived from the frequency list of words and control the distribution of words and their respective categories over the texts. While methods in quantitative content analysis in this way transform observations of found categories into quantitative statistical data, the qualitative content analysis focuses more on the intentionality and its implications. There are strong parallels between qualitative content analysis and <a href=\"/wiki/Thematic_analysis\" title=\"Thematic analysis\">thematic analysis</a>.<sup class=\"reference\" id=\"cite_ref-6\"><a href=\"#cite_note-6\">[6]</a></sup>\n",
      "</p>, <p>Quantitative content analysis highlights frequency counts and statistical analysis of these coded frequencies.<sup class=\"reference\" id=\"cite_ref-:03_7-0\"><a href=\"#cite_note-:03-7\">[7]</a></sup> Additionally, quantitative content analysis begins with a framed hypothesis with coding decided on before the analysis begins. These coding categories are strictly relevant to the researcher's hypothesis. Quantitative analysis also takes a deductive approach.<sup class=\"reference\" id=\"cite_ref-:12_8-0\"><a href=\"#cite_note-:12-8\">[8]</a></sup> Examples of content-analytical variables and constructs can be found, for example, in the open-access database <a class=\"external text\" href=\"https://www.hope.uzh.ch/doca\" rel=\"nofollow\">DOCA</a>. This database compiles, systematizes, and evaluates relevant content-analytical variables of communication and political science research areas and topics.\n",
      "</p>, <p><a href=\"/wiki/Siegfried_Kracauer\" title=\"Siegfried Kracauer\">Siegfried Kracauer</a> provides a critique of quantitative analysis, asserting that it oversimplifies complex communications in order to be more reliable. On the other hand, qualitative analysis deals with the intricacies of latent interpretations, whereas quantitative has a focus on manifest meanings. He also acknowledges an \"overlap\" of qualitative and quantitative content analysis.<sup class=\"reference\" id=\"cite_ref-:03_7-1\"><a href=\"#cite_note-:03-7\">[7]</a></sup> Patterns are looked at more closely in qualitative analysis, and based on the latent meanings that the researcher may find, the course of the research could be changed. It is inductive and begins with open research questions, as opposed to a hypothesis.<sup class=\"reference\" id=\"cite_ref-:12_8-1\"><a href=\"#cite_note-:12-8\">[8]</a></sup>\n",
      "</p>, <p>The data collection instrument used in content analysis is the codebook or coding scheme. In qualitative content analysis the codebook is constructed and improved <i>during</i> coding, while in quantitative content analysis the codebook needs to be developed and pretested for reliability and validity <i>before</i> coding.<sup class=\"reference\" id=\"cite_ref-Neuendorf2016_4-1\"><a href=\"#cite_note-Neuendorf2016-4\">[4]</a></sup> The codebook includes detailed instructions for human coders plus clear definitions of the respective concepts or variables to be coded plus the assigned values.\n",
      "</p>, <p>According to current standards of good scientific practice, each content analysis study should provide their codebook in the appendix or as supplementary material so that <a href=\"/wiki/Reproducibility\" title=\"Reproducibility\">reproducibility</a> of the study is ensured. On the <a class=\"external text\" href=\"https://osf.io\" rel=\"nofollow\">Open Science Framework</a> (OSF) server of the <a href=\"/wiki/Center_for_Open_Science\" title=\"Center for Open Science\">Center for Open Science</a> a lot of codebooks of content analysis studies are freely available via search for \"codebook\".\n",
      "</p>, <p>Furthermore, the <i>Database of Variables for Content Analysis</i> (DOCA) provides an open access archive of pretested variables and established codebooks for content analyses.<sup class=\"reference\" id=\"cite_ref-9\"><a href=\"#cite_note-9\">[9]</a></sup> Measures from the archive can be adopted in future studies to ensure the use of high-quality and comparable instruments. DOCA covers, among others, measures for the content analysis of fictional media and entertainment (e.g., measures for sexualization in video games<sup class=\"reference\" id=\"cite_ref-10\"><a href=\"#cite_note-10\">[10]</a></sup>), of user-generated media content (e.g., measures for online hate speech<sup class=\"reference\" id=\"cite_ref-11\"><a href=\"#cite_note-11\">[11]</a></sup>), and of news media and journalism (e.g., measures for stock photo use in press reporting on child sexual abuse,<sup class=\"reference\" id=\"cite_ref-12\"><a href=\"#cite_note-12\">[12]</a></sup> and measures of personalization in election campaign coverage<sup class=\"reference\" id=\"cite_ref-13\"><a href=\"#cite_note-13\">[13]</a></sup>).\n",
      "</p>, <p>With the rise of common computing facilities like PCs, computer-based methods of analysis are growing in popularity.<sup class=\"reference\" id=\"cite_ref-14\"><a href=\"#cite_note-14\">[14]</a></sup><sup class=\"reference\" id=\"cite_ref-15\"><a href=\"#cite_note-15\">[15]</a></sup><sup class=\"reference\" id=\"cite_ref-16\"><a href=\"#cite_note-16\">[16]</a></sup> Answers to open ended questions, newspaper articles, political party manifestos, medical records or systematic observations in experiments can all be subject to systematic analysis of textual data.\n",
      "</p>, <p>By having contents of communication available in form of machine readable texts, the input is analyzed for frequencies and coded into categories for building up inferences.\n",
      "</p>, <p>Computer-assisted analysis can help with large, electronic data sets by cutting out time and eliminating the need for multiple human coders to establish inter-coder reliability. However, human coders can still be employed for content analysis, as they are often more able to pick out nuanced and latent meanings in text. A study  found that human coders were able to evaluate a broader range and make inferences based on latent meanings.<sup class=\"reference\" id=\"cite_ref-17\"><a href=\"#cite_note-17\">[17]</a></sup>\n",
      "</p>, <p>Robert Weber notes: \"To make valid inferences from the text, it is important that the classification procedure be reliable in the sense of being consistent: Different people should code the same text in the same way\".<sup class=\"reference\" id=\"cite_ref-18\"><a href=\"#cite_note-18\">[18]</a></sup> The validity, inter-coder reliability and intra-coder reliability are subject to intense methodological research efforts over long years.<sup class=\"reference\" id=\"cite_ref-Krippendorff2004_5-1\"><a href=\"#cite_note-Krippendorff2004-5\">[5]</a></sup>\n",
      "Neuendorf suggests that when human coders are used in content analysis at least two independent coders should be used. <a href=\"/wiki/Reliability_(statistics)\" title=\"Reliability (statistics)\">Reliability</a> of human coding is often measured using a statistical measure of <i>inter-coder reliability</i> or \"the amount of agreement or correspondence among two or more coders\".<sup class=\"reference\" id=\"cite_ref-Neuendorf2016_4-2\"><a href=\"#cite_note-Neuendorf2016-4\">[4]</a></sup> Lacy and Riffe identify the measurement of inter-coder reliability as a strength of quantitative content analysis, arguing that, if content analysts do not measure inter-coder reliability, their data are no more reliable than the subjective impressions of a single reader.<sup class=\"reference\" id=\"cite_ref-19\"><a href=\"#cite_note-19\">[19]</a></sup>\n",
      "</p>, <p>According to today's reporting standards, quantitative content analyses should be published with complete codebooks and for all variables or measures in the codebook the appropriate inter-coder or <a href=\"/wiki/Inter-rater_reliability\" title=\"Inter-rater reliability\">inter-rater reliability</a> coefficients should be reported based on empirical pre-tests.<sup class=\"reference\" id=\"cite_ref-Neuendorf2016_4-3\"><a href=\"#cite_note-Neuendorf2016-4\">[4]</a></sup><sup class=\"reference\" id=\"cite_ref-:1_20-0\"><a href=\"#cite_note-:1-20\">[20]</a></sup><sup class=\"reference\" id=\"cite_ref-21\"><a href=\"#cite_note-21\">[21]</a></sup> Furthermore, the <a href=\"/wiki/Validity_(statistics)\" title=\"Validity (statistics)\">validity</a> of all variables or measures in the codebook must be ensured. This can be achieved through the use of established measures that have proven their validity in earlier studies. Also, the <a href=\"/wiki/Validity_(statistics)#Content_validity\" title=\"Validity (statistics)\">content validity</a> of the measures can be checked by experts from the field who scrutinize and then approve or correct coding instructions, definitions and examples in the codebook.\n",
      "</p>, <p>There are five types of texts in content analysis:\n",
      "</p>, <p>Content analysis is research using the categorization and classification of speech, written text, interviews, images, or other forms of communication.  In its beginnings, using the first newspapers at the end of the 19th century, analysis was done manually by measuring the number of columns given a subject. The approach can also be traced back to a university student studying patterns in Shakespeare's literature in 1893.<sup class=\"reference\" id=\"cite_ref-22\"><a href=\"#cite_note-22\">[22]</a></sup>\n",
      "</p>, <p>Over the years, content analysis has been applied to a variety of scopes. <a href=\"/wiki/Hermeneutics\" title=\"Hermeneutics\">Hermeneutics</a> and <a href=\"/wiki/Philology\" title=\"Philology\">philology</a> have long used content analysis to interpret sacred and profane texts and, in many cases, to attribute texts' <a class=\"mw-redirect\" href=\"/wiki/Authorship\" title=\"Authorship\">authorship</a> and <a href=\"/wiki/Authentication\" title=\"Authentication\">authenticity</a>.<sup class=\"reference\" id=\"cite_ref-Tipaldo_2014_42_3-1\"><a href=\"#cite_note-Tipaldo_2014_42-3\">[3]</a></sup><sup class=\"reference\" id=\"cite_ref-Krippendorff2004_5-2\"><a href=\"#cite_note-Krippendorff2004-5\">[5]</a></sup>\n",
      "</p>, <p>In recent times, particularly with the advent of <a href=\"/wiki/Mass_communication\" title=\"Mass communication\">mass communication</a>, content analysis has known an increasing use to deeply analyze and understand media content and media logic. \n",
      "The political scientist <a href=\"/wiki/Harold_Lasswell\" title=\"Harold Lasswell\">Harold Lasswell</a> formulated the core questions of content analysis in its early-mid 20th-century mainstream version: \"Who says what, to whom, why, to what extent and with what effect?\".<sup class=\"reference\" id=\"cite_ref-23\"><a href=\"#cite_note-23\">[23]</a></sup> The strong emphasis for a quantitative approach started up by Lasswell was finally carried out by another \"father\" of content analysis, <a href=\"/wiki/Bernard_Berelson\" title=\"Bernard Berelson\">Bernard Berelson</a>, who proposed a definition of content analysis which, from this point of view, is emblematic: \"a research technique for the objective, systematic and quantitative description of the manifest content of communication\".<sup class=\"reference\" id=\"cite_ref-Berelson1952_24-0\"><a href=\"#cite_note-Berelson1952-24\">[24]</a></sup>\n",
      "</p>, <p>Quantitative content analysis has enjoyed a renewed popularity in recent years thanks to technological advances and fruitful application in of mass communication and personal communication research. Content analysis of textual <a href=\"/wiki/Big_data\" title=\"Big data\">big data</a> produced by <a href=\"/wiki/New_media\" title=\"New media\">new media</a>, particularly <a href=\"/wiki/Social_media\" title=\"Social media\">social media</a> and <a class=\"mw-redirect\" href=\"/wiki/Mobile_devices\" title=\"Mobile devices\">mobile devices</a> has become popular. These approaches take a simplified view of language that ignores the complexity of <a href=\"/wiki/Semiosis\" title=\"Semiosis\">semiosis</a>, the process by which meaning is formed out of language. Quantitative content analysts have been criticized for limiting the scope of content analysis to simple counting, and for applying the measurement methodologies of the natural sciences without reflecting critically on their appropriateness to social science.<sup class=\"reference\" id=\"cite_ref-:0_25-0\"><a href=\"#cite_note-:0-25\">[25]</a></sup> Conversely, qualitative content analysts have been criticized for being insufficiently systematic and too impressionistic.<sup class=\"reference\" id=\"cite_ref-:0_25-1\"><a href=\"#cite_note-:0-25\">[25]</a></sup> Krippendorff argues that quantitative and qualitative approaches to content analysis tend to overlap, and that there can be no generalisable conclusion as to which approach is superior.<sup class=\"reference\" id=\"cite_ref-:0_25-2\"><a href=\"#cite_note-:0-25\">[25]</a></sup>\n",
      "</p>, <p>Content analysis can also be described as studying <a href=\"/wiki/Trace_evidence\" title=\"Trace evidence\">traces</a>, which are documents from past times, and artifacts, which are non-linguistic documents. Texts are understood to be produced by communication processes in a broad sense of that phrase—often gaining mean through <a href=\"/wiki/Abductive_reasoning\" title=\"Abductive reasoning\">abduction</a>.<sup class=\"reference\" id=\"cite_ref-Tipaldo_2014_42_3-2\"><a href=\"#cite_note-Tipaldo_2014_42-3\">[3]</a></sup><sup class=\"reference\" id=\"cite_ref-26\"><a href=\"#cite_note-26\">[26]</a></sup>\n",
      "</p>, <p>Manifest content is readily understandable at its face value. Its meaning is direct. Latent content is not as overt, and requires interpretation to uncover the meaning or implication.<sup class=\"reference\" id=\"cite_ref-27\"><a href=\"#cite_note-27\">[27]</a></sup>\n",
      "</p>, <p>Holsti groups fifteen uses of content analysis into three basic <a class=\"mw-redirect\" href=\"/wiki/Categorisation\" title=\"Categorisation\">categories</a>:<sup class=\"reference\" id=\"cite_ref-Holsti1969_28-0\"><a href=\"#cite_note-Holsti1969-28\">[28]</a></sup>\n",
      "</p>, <p>He also places these uses into the context of the basic communication <a href=\"/wiki/Paradigm\" title=\"Paradigm\">paradigm</a>.\n",
      "</p>, <p>The following table shows fifteen uses of content analysis in terms of their general purpose, element of the communication paradigm to which they apply, and the general question they are intended to answer.\n",
      "</p>, <p>As a counterpoint, there are limits to the scope of use for the procedures that characterize content analysis. In particular, if access to the goal of analysis can be obtained by direct means without material interference, then direct measurement techniques yield better data.<sup class=\"reference\" id=\"cite_ref-30\"><a href=\"#cite_note-30\">[30]</a></sup> Thus, while content analysis attempts to quantifiably describe <i>communications</i> whose features are primarily categorical——limited usually to a nominal or ordinal scale——via selected conceptual units (the <i>unitization</i>) which are assigned values (the <i>categorization</i>) for <i>enumeration</i> while monitoring <i>intercoder reliability</i>, if instead the target quantity manifestly is already directly measurable——typically on an interval or ratio scale——especially a continuous physical quantity, then such targets usually are not listed among those needing the \"subjective\" selections and formulations of content analysis.<sup class=\"reference\" id=\"cite_ref-31\"><a href=\"#cite_note-31\">[31]</a></sup><sup class=\"reference\" id=\"cite_ref-32\"><a href=\"#cite_note-32\">[32]</a></sup><sup class=\"reference\" id=\"cite_ref-33\"><a href=\"#cite_note-33\">[33]</a></sup><sup class=\"reference\" id=\"cite_ref-34\"><a href=\"#cite_note-34\">[34]</a></sup><sup class=\"reference\" id=\"cite_ref-35\"><a href=\"#cite_note-35\">[35]</a></sup><sup class=\"reference\" id=\"cite_ref-36\"><a href=\"#cite_note-36\">[36]</a></sup><sup class=\"reference\" id=\"cite_ref-:1_20-1\"><a href=\"#cite_note-:1-20\">[20]</a></sup><sup class=\"reference\" id=\"cite_ref-37\"><a href=\"#cite_note-37\">[37]</a></sup> For example (from mixed research and clinical application), as medical images <i>communicate</i> diagnostic features to physicians, <a href=\"/wiki/Neuroimaging\" title=\"Neuroimaging\">neuroimaging</a>'s <a href=\"/wiki/Stroke\" title=\"Stroke\">stroke</a> (infarct) volume scale called ASPECTS is <i>unitized</i> as 10 qualitatively delineated (unequal) brain regions in the <a href=\"/wiki/Middle_cerebral_artery\" title=\"Middle cerebral artery\">middle cerebral artery</a> territory, which it <i>categorizes</i> as being at least partly versus not at all infarcted in order to <i>enumerate</i> the latter, with published series often assessing <i>intercoder reliability</i> by <a href=\"/wiki/Cohen%27s_kappa\" title=\"Cohen's kappa\">Cohen's kappa</a>. The foregoing <i>italicized operations</i> impose the uncredited <i>form</i> of content analysis onto an estimation of infarct extent, which instead is easily enough and more accurately measured as a volume directly on the images.<sup class=\"reference\" id=\"cite_ref-38\"><a href=\"#cite_note-38\">[38]</a></sup><sup class=\"reference\" id=\"cite_ref-39\"><a href=\"#cite_note-39\">[39]</a></sup> (\"Accuracy ... is the highest form of reliability.\"<sup class=\"reference\" id=\"cite_ref-40\"><a href=\"#cite_note-40\">[40]</a></sup>) The concomitant clinical assessment, however, by the <a href=\"/wiki/National_Institutes_of_Health_Stroke_Scale\" title=\"National Institutes of Health Stroke Scale\">National Institutes of Health Stroke Scale</a> (NIHSS) or the <a href=\"/wiki/Modified_Rankin_Scale\" title=\"Modified Rankin Scale\">modified Rankin Scale</a> (mRS), retains the necessary form of content analysis. Recognizing potential limits of content analysis across the contents of language and images alike, <a href=\"/wiki/Klaus_Krippendorff\" title=\"Klaus Krippendorff\">Klaus Krippendorff</a> affirms that \"comprehen[sion] ... may ... not conform at all to the process of classification and/or counting by which most content analyses proceed,\"<sup class=\"reference\" id=\"cite_ref-41\"><a href=\"#cite_note-41\">[41]</a></sup> suggesting that content analysis might materially distort a message.\n",
      "</p>, <p>The process of the initial coding scheme or approach to coding is contingent on the particular content analysis approach selected. Through a directed content analysis, the scholars draft a preliminary coding scheme from pre-existing theory or assumptions. While with the conventional content analysis approach, the initial coding scheme developed from the data.\n",
      "</p>, <p>With either approach above, immersing oneself into the data to obtain an overall picture is recommendable for researchers to conduct. Furthermore, identifying a consistent and clear unit of coding is vital, and researchers' choices range from a single word to several paragraphs, from texts to iconic symbols. Last, constructing the relationships between codes by sorting out them within specific categories or themes.<sup class=\"reference\" id=\"cite_ref-42\"><a href=\"#cite_note-42\">[42]</a></sup>\n",
      "</p>]\n"
     ]
    }
   ],
   "source": [
    "print(contentPTags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T04:59:16.476117Z",
     "start_time": "2024-01-10T04:59:16.470526Z"
    }
   },
   "source": [
    "Another excursion: Why do we use enumerate() here? enumerate() takes a collection, enumerates, and returns an enumate object with both the numbers and the collection. For example, contentPTags (the collection we used here) is comprised of paragraphs. We want the paragraph number of each paragraph. And this is what enumerate() does: it returns the paragraph number and the paragraph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be adding these new texts to our DataFrame `contentParagraphsDF` so we\n",
    "will need to add 2 more columns to keep track of paragraph numbers and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:54.130412Z",
     "start_time": "2024-01-11T05:20:54.127077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                       paragraph-text  \\\n0   Content analysis is the study of documents and...   \n1   Practices and philosophies of content analysis...   \n2   Computers are increasingly used in content ana...   \n3   Content analysis is best understood as a broad...   \n4   The simplest and most objective form of conten...   \n5   A further step in analysis is the distinction ...   \n6   Quantitative content analysis highlights frequ...   \n7   Siegfried Kracauer provides a critique of quan...   \n8   The data collection instrument used in content...   \n9   According to current standards of good scienti...   \n10  Furthermore, the Database of Variables for Con...   \n11  With the rise of common computing facilities l...   \n12  By having contents of communication available ...   \n13  Computer-assisted analysis can help with large...   \n14  Robert Weber notes: \"To make valid inferences ...   \n15  According to today's reporting standards, quan...   \n16  There are five types of texts in content analy...   \n17  Content analysis is research using the categor...   \n18  Over the years, content analysis has been appl...   \n19  In recent times, particularly with the advent ...   \n20  Quantitative content analysis has enjoyed a re...   \n21  Content analysis can also be described as stud...   \n22  Manifest content is readily understandable at ...   \n23  Holsti groups fifteen uses of content analysis...   \n24  He also places these uses into the context of ...   \n25  The following table shows fifteen uses of cont...   \n26  As a counterpoint, there are limits to the sco...   \n27  The process of the initial coding scheme or ap...   \n28  With either approach above, immersing oneself ...   \n\n                                            source  paragraph-number  \n0   https://en.wikipedia.org/wiki/Content_analysis                 0  \n1   https://en.wikipedia.org/wiki/Content_analysis                 1  \n2   https://en.wikipedia.org/wiki/Content_analysis                 2  \n3   https://en.wikipedia.org/wiki/Content_analysis                 3  \n4   https://en.wikipedia.org/wiki/Content_analysis                 4  \n5   https://en.wikipedia.org/wiki/Content_analysis                 5  \n6   https://en.wikipedia.org/wiki/Content_analysis                 6  \n7   https://en.wikipedia.org/wiki/Content_analysis                 7  \n8   https://en.wikipedia.org/wiki/Content_analysis                 8  \n9   https://en.wikipedia.org/wiki/Content_analysis                 9  \n10  https://en.wikipedia.org/wiki/Content_analysis                10  \n11  https://en.wikipedia.org/wiki/Content_analysis                11  \n12  https://en.wikipedia.org/wiki/Content_analysis                12  \n13  https://en.wikipedia.org/wiki/Content_analysis                13  \n14  https://en.wikipedia.org/wiki/Content_analysis                14  \n15  https://en.wikipedia.org/wiki/Content_analysis                15  \n16  https://en.wikipedia.org/wiki/Content_analysis                16  \n17  https://en.wikipedia.org/wiki/Content_analysis                17  \n18  https://en.wikipedia.org/wiki/Content_analysis                18  \n19  https://en.wikipedia.org/wiki/Content_analysis                19  \n20  https://en.wikipedia.org/wiki/Content_analysis                20  \n21  https://en.wikipedia.org/wiki/Content_analysis                21  \n22  https://en.wikipedia.org/wiki/Content_analysis                22  \n23  https://en.wikipedia.org/wiki/Content_analysis                23  \n24  https://en.wikipedia.org/wiki/Content_analysis                24  \n25  https://en.wikipedia.org/wiki/Content_analysis                25  \n26  https://en.wikipedia.org/wiki/Content_analysis                26  \n27  https://en.wikipedia.org/wiki/Content_analysis                27  \n28  https://en.wikipedia.org/wiki/Content_analysis                28  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paragraph-text</th>\n      <th>source</th>\n      <th>paragraph-number</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Content analysis is the study of documents and...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Practices and philosophies of content analysis...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Computers are increasingly used in content ana...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Content analysis is best understood as a broad...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The simplest and most objective form of conten...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>A further step in analysis is the distinction ...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Quantitative content analysis highlights frequ...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Siegfried Kracauer provides a critique of quan...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>The data collection instrument used in content...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>According to current standards of good scienti...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Furthermore, the Database of Variables for Con...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>With the rise of common computing facilities l...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>By having contents of communication available ...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Computer-assisted analysis can help with large...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Robert Weber notes: \"To make valid inferences ...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>According to today's reporting standards, quan...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>There are five types of texts in content analy...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Content analysis is research using the categor...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Over the years, content analysis has been appl...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>In recent times, particularly with the advent ...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Quantitative content analysis has enjoyed a re...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Content analysis can also be described as stud...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Manifest content is readily understandable at ...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Holsti groups fifteen uses of content analysis...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>He also places these uses into the context of ...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>The following table shows fifteen uses of cont...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>As a counterpoint, there are limits to the sco...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>The process of the initial coding scheme or ap...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>With either approach above, immersing oneself ...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contentParagraphsDF['source'] = [wikipedia_content_analysis] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['paragraph-number'] = range(len(contentParagraphsDF['paragraph-text']))\n",
    "\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T04:59:16.510180Z",
     "start_time": "2024-01-10T04:59:16.474443Z"
    }
   },
   "source": [
    "Then we can add two more columns to our `Dataframe` and define a function to\n",
    "parse\n",
    "each linked page and add its text to our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:54.156350Z",
     "start_time": "2024-01-11T05:20:54.132251Z"
    }
   },
   "outputs": [],
   "source": [
    "contentParagraphsDF['source-paragraph-number'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['source-paragraph-text'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "\n",
    "def getTextFromWikiPage(targetURL, sourceParNum, sourceText):\n",
    "    #Make a dict to store data before adding it to the DataFrame\n",
    "    parsDict = {'source' : [], 'paragraph-number' : [], 'paragraph-text' : [], 'source-paragraph-number' : [],  'source-paragraph-text' : []}\n",
    "    #Now we get the page\n",
    "    r = requests.get(targetURL)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    #enumerating gives use the paragraph number\n",
    "    for parNum, pTag in enumerate(soup.body.findAll('p')):\n",
    "        #same regex as before\n",
    "        parsDict['paragraph-text'].append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "        parsDict['paragraph-number'].append(parNum)\n",
    "        parsDict['source'].append(targetURL)\n",
    "        parsDict['source-paragraph-number'].append(sourceParNum)\n",
    "        parsDict['source-paragraph-text'].append(sourceText)\n",
    "    return pandas.DataFrame(parsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T04:59:16.511020Z",
     "start_time": "2024-01-10T04:59:16.485563Z"
    }
   },
   "source": [
    "And run it on our list of link tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:20:55.004038Z",
     "start_time": "2024-01-11T05:20:54.134777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                       paragraph-text  \\\n0   Content analysis is the study of documents and...   \n1   Practices and philosophies of content analysis...   \n2   Computers are increasingly used in content ana...   \n3   Content analysis is best understood as a broad...   \n4   The simplest and most objective form of conten...   \n..                                                ...   \n58  Much of qualitative coding can be attributed t...   \n59  Coding is considered a process of discovery an...   \n60  The process can be done manually, which can be...   \n61  After assembling codes it is time to organize ...   \n62  Creating memos during the coding process is in...   \n\n                                               source  paragraph-number  \\\n0      https://en.wikipedia.org/wiki/Content_analysis                 0   \n1      https://en.wikipedia.org/wiki/Content_analysis                 1   \n2      https://en.wikipedia.org/wiki/Content_analysis                 2   \n3      https://en.wikipedia.org/wiki/Content_analysis                 3   \n4      https://en.wikipedia.org/wiki/Content_analysis                 4   \n..                                                ...               ...   \n58  https://en.wikipedia.org/wiki/Coding_(social_s...                 8   \n59  https://en.wikipedia.org/wiki/Coding_(social_s...                 9   \n60  https://en.wikipedia.org/wiki/Coding_(social_s...                10   \n61  https://en.wikipedia.org/wiki/Coding_(social_s...                11   \n62  https://en.wikipedia.org/wiki/Coding_(social_s...                12   \n\n   source-paragraph-number                     source-paragraph-text  \n0                     None                                      None  \n1                     None                                      None  \n2                     None                                      None  \n3                     None                                      None  \n4                     None                                      None  \n..                     ...                                       ...  \n58                       1  assigned labels (sometimes called codes)  \n59                       1  assigned labels (sometimes called codes)  \n60                       1  assigned labels (sometimes called codes)  \n61                       1  assigned labels (sometimes called codes)  \n62                       1  assigned labels (sometimes called codes)  \n\n[63 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paragraph-text</th>\n      <th>source</th>\n      <th>paragraph-number</th>\n      <th>source-paragraph-number</th>\n      <th>source-paragraph-text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Content analysis is the study of documents and...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>0</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Practices and philosophies of content analysis...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>1</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Computers are increasingly used in content ana...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>2</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Content analysis is best understood as a broad...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>3</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The simplest and most objective form of conten...</td>\n      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n      <td>4</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>Much of qualitative coding can be attributed t...</td>\n      <td>https://en.wikipedia.org/wiki/Coding_(social_s...</td>\n      <td>8</td>\n      <td>1</td>\n      <td>assigned labels (sometimes called codes)</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>Coding is considered a process of discovery an...</td>\n      <td>https://en.wikipedia.org/wiki/Coding_(social_s...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>assigned labels (sometimes called codes)</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>The process can be done manually, which can be...</td>\n      <td>https://en.wikipedia.org/wiki/Coding_(social_s...</td>\n      <td>10</td>\n      <td>1</td>\n      <td>assigned labels (sometimes called codes)</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>After assembling codes it is time to organize ...</td>\n      <td>https://en.wikipedia.org/wiki/Coding_(social_s...</td>\n      <td>11</td>\n      <td>1</td>\n      <td>assigned labels (sometimes called codes)</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>Creating memos during the coding process is in...</td>\n      <td>https://en.wikipedia.org/wiki/Coding_(social_s...</td>\n      <td>12</td>\n      <td>1</td>\n      <td>assigned labels (sometimes called codes)</td>\n    </tr>\n  </tbody>\n</table>\n<p>63 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for urlTuple in otherPAgeURLS[:3]:\n",
    "    #ignore_index means the indices will not be reset after each append\n",
    "    contentParagraphsDF = pd.concat([contentParagraphsDF, pd.DataFrame(getTextFromWikiPage(*urlTuple))],ignore_index=True)\n",
    "    #contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T04:59:16.661106Z",
     "start_time": "2024-01-10T04:59:16.488640Z"
    }
   },
   "source": [
    "\n",
    "# <font color=\"white\">Exercise 2</font>\n",
    "<font color=\"white\">Construct cells immediately below this that spider webcontent from another site with content relating to your anticipated final project. Specifically, identify urls on a core page, then follow and extract content from them into a pandas `Dataframe`. In addition, demonstrate a *recursive* spider, which follows more than one level of links (i.e., follows links from a site, then follows links on followed sites to new sites, etc.), making sure to define a reasonable endpoint so that you do not wander the web forever :-).</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "data": {
      "text/plain": "                           news_title          news_sub_title layout_num  \\\n0                “一江一河”滨水空间打造更多有温度的服务     带“汪汪队”逛店 与“毛孩子”友好同行          5   \n1                “一江一河”滨水空间打造更多有温度的服务     带“汪汪队”逛店 与“毛孩子”友好同行          5   \n2                                            300余家企业已签约第七届进博会          1   \n3           调查显示上海公众安全感和满意度连续11年“双提升”        “小目标”串起千家万户“大平安”          1   \n4                                                  百年老大楼迎来新传奇          1   \n5   陈吉宁：更好担负起新的文化使命  奋力开创国际文化大都市建设新局面          打造习近平文化思想最佳实践地          2   \n6                      我国成功发射爱因斯坦探针卫星          天体爆发“捕手”启程探秘黑洞          2   \n7                                              良性互动，让大学校园更好开放          2   \n8                                                 崇明水仙文化旅游节举行          2   \n9                                            30名上海市优秀企业家上午获表彰          2   \n10                                             让残疾人毕业生就业“无障碍”          3   \n11              老伯误饮曼陀罗药酒致意识不清，医生提醒——          擅用植物泡酒泡茶  当心中毒          3   \n12            外滩将有新地标，老市府大楼更新项目3月竣工——         百年老大楼全球招商，底气何在？          4   \n13              上海交大启动“邹韬奋卓越新闻导师领航计划”          指导培训有志学子投身新闻事业          6   \n14                                                        黑皮子          6   \n15        过山车、漂流要不要关？ 雨天怎么巡游？ 烟花如何燃放？     “疯狂动物城”气象服务守护游客玩乐安全          6   \n16                                             最毒乳腺癌靶向精准治疗获突破          6   \n17                                             “撤场前一天还在收钱充卡！”          7   \n18                                           静安：社会组织联合会赴疆对口援建          7   \n19                                       奉贤：“东方美谷·风雨彩虹”圆梦行动启动          7   \n20               “蓝天下的至爱”爱心打卡第三阶段活动开启         “手拉手”结对助学项目邀您参与          7   \n21                                                  春运火车票后天起售          8   \n22                                     东航C919 首次执飞上海虹桥—北京大兴航线          8   \n23                                            铁路上海站今起实施新列车运行图          8   \n24          调查显示上海公众安全感和满意度连续11年“双提升”        “小目标”串起千家万户“大平安”          8   \n25                                                     不握手的法医          9   \n26                                        伤亡惨重  加沙经历“最血腥24小时”         10   \n27                                                美反对驱离加沙居民计划         10   \n28                    任命34岁阿塔尔担任法国新总理            马克龙选了“马克龙男孩”         10   \n29                                                         广告         10   \n30                                          防长进了ICU  拜登三天后才知情         10   \n31               人性成为光芒，乡音成全热爱，精雕成就经典          《繁花》落尽时 何以香如故？         11   \n32                                            《繁花》舞台剧 卖到服务器崩溃         11   \n33                 上海足坛的昔日“繁花”点评“范厂长”               外表粗犷 内心细致         11   \n34                                                  经典黄梅戏赢得掌声         12   \n35                                                 上海女排挺进排超决赛         12   \n36                                                   海派京剧获得赞誉         12   \n37                  F1中国大奖赛昨晚开票，周冠宇现身          “在上海，目标不只是拿积分”         12   \n38                                                    从小说到电视剧         13   \n39                                         双双燕·夜访张玉娘诗文馆有感用梅溪韵         13   \n40                                                    日有吉（书法）         13   \n41                                                  关于长寿的一些思考         13   \n42                                                       山村年味         13   \n43                                                      南浔三碗茶         13   \n44                                                   朋友圈是一条河流         14   \n45                                                   日落额尔齐斯河上         14   \n46                                                  “显而易见”很重要         14   \n47                                                    生命的三分之一         14   \n48                                                        小摇铃         14   \n49                                                       乘机杂忆         14   \n50                                                    寻“拉猫”不遇         14   \n51                                               徐悲鸿与淮海坊的时间碎片         15   \n52                                                奉贤通津桥静守时光流转         15   \n53                                                把脉2024年中国车市         16   \n54                                              上海私家车牌竞拍中标率大升         16   \n55                                       深耕价值营销  林肯在华迎来50万名用户         16   \n\n   layout_name        date  year month day     authors  \\\n0         上海新闻  2024-01-10  2024    01  10  [杨玉红, 钱文婷]   \n1         上海新闻  2024-01-10  2024    01  10  [杨玉红, 钱文婷]   \n2         一版要闻  2024-01-10  2024    01  10          []   \n3         一版要闻  2024-01-10  2024    01  10       [潘高峰]   \n4         一版要闻  2024-01-10  2024    01  10          []   \n5           要闻  2024-01-10  2024    01  10          []   \n6           要闻  2024-01-10  2024    01  10          []   \n7           要闻  2024-01-10  2024    01  10          []   \n8           要闻  2024-01-10  2024    01  10          []   \n9           要闻  2024-01-10  2024    01  10          []   \n10        综合新闻  2024-01-10  2024    01  10          []   \n11        综合新闻  2024-01-10  2024    01  10          []   \n12          焦点  2024-01-10  2024    01  10       [姚丽萍]   \n13        上海新闻  2024-01-10  2024    01  10          []   \n14        上海新闻  2024-01-10  2024    01  10          []   \n15        上海新闻  2024-01-10  2024    01  10          []   \n16        上海新闻  2024-01-10  2024    01  10          []   \n17         帮侬忙  2024-01-10  2024    01  10          []   \n18         帮侬忙  2024-01-10  2024    01  10          []   \n19         帮侬忙  2024-01-10  2024    01  10          []   \n20         帮侬忙  2024-01-10  2024    01  10          []   \n21        综合新闻  2024-01-10  2024    01  10          []   \n22        综合新闻  2024-01-10  2024    01  10          []   \n23        综合新闻  2024-01-10  2024    01  10          []   \n24        综合新闻  2024-01-10  2024    01  10       [潘高峰]   \n25          特稿  2024-01-10  2024    01  10       [潘高峰]   \n26        国际新闻  2024-01-10  2024    01  10          []   \n27        国际新闻  2024-01-10  2024    01  10          []   \n28        国际新闻  2024-01-10  2024    01  10          []   \n29        国际新闻  2024-01-10  2024    01  10          []   \n30        国际新闻  2024-01-10  2024    01  10          []   \n31        文体新闻  2024-01-10  2024    01  10          []   \n32        文体新闻  2024-01-10  2024    01  10          []   \n33        文体新闻  2024-01-10  2024    01  10          []   \n34        文体新闻  2024-01-10  2024    01  10          []   \n35        文体新闻  2024-01-10  2024    01  10          []   \n36        文体新闻  2024-01-10  2024    01  10          []   \n37        文体新闻  2024-01-10  2024    01  10          []   \n38         夜光杯  2024-01-10  2024    01  10          []   \n39         夜光杯  2024-01-10  2024    01  10          []   \n40         夜光杯  2024-01-10  2024    01  10          []   \n41         夜光杯  2024-01-10  2024    01  10          []   \n42         夜光杯  2024-01-10  2024    01  10          []   \n43         夜光杯  2024-01-10  2024    01  10          []   \n44         夜光杯  2024-01-10  2024    01  10          []   \n45         夜光杯  2024-01-10  2024    01  10          []   \n46         夜光杯  2024-01-10  2024    01  10          []   \n47         夜光杯  2024-01-10  2024    01  10          []   \n48         夜光杯  2024-01-10  2024    01  10          []   \n49         夜光杯  2024-01-10  2024    01  10          []   \n50         夜光杯  2024-01-10  2024    01  10          []   \n51        新民旅游  2024-01-10  2024    01  10          []   \n52        新民旅游  2024-01-10  2024    01  10          []   \n53     汽车周刊/专题  2024-01-10  2024    01  10          []   \n54     汽车周刊/专题  2024-01-10  2024    01  10          []   \n55     汽车周刊/专题  2024-01-10  2024    01  10          []   \n\n                                              article photographers  \n0   周末带“毛孩子”去哪里撒欢，成为申城不少爱宠人士交流的热门话题。家门口的滨水公共空间成为不少...     [周馨, 钱文婷]  \n1   周末带“毛孩子”去哪里撒欢，成为申城不少爱宠人士交流的热门话题。家门口的滨水公共空间成为不少...     [周馨, 钱文婷]  \n2   活动现场，百事公司、美商美乐家、拜耳、特斯拉、3M、葛兰素史克等23家企业集体签约第七届进博...            []  \n3   第三方调查显示：2023年，上海公众安全感和对公安工作满意度，从2013年至今已连续11年实...          [张龙]  \n4   这里，是上海市第一面五星红旗升起地，是新中国上海市第一任市长陈毅同志的办公地。\\n    这...            []  \n5   李文堂的报告围绕学习习近平文化思想，理论联系实际，历史与现实相结合，从担当文化使命与文明使命...            []  \n6   爱因斯坦探针卫星由中国科学院微小卫星创新研究院抓总研制，是中国科学院空间科学先导专项继“悟空...            []  \n7   “哈工大学霸”人手一个“橘宝宝”——来自广西南宁的11位萌娃近日在东北各城市开展“冰雪研学之...            []  \n8   活动期间将举办花卉游园会，让传统民俗文化与新发展理念碰撞交流，推动花卉产业高质量发展，市民可...          [徐程]  \n9   由上海市经济和信息化委员会、上海市国有资产管理监督委员会为指导单位，上海市企业联合会、上海市...            []  \n10  华东师大学生小蒋目前正在准备毕业论文，是一篇关于视障大学生就业现状的调查。调查发现，对于健全...            []  \n11  误饮一小杯药酒后中毒\\n    家住金山本地的林老伯常年腰腿疼痛不适，于是自行泡制了曼陀罗药...            []  \n12  外滩，将有新地标。\\n    百年老市府大楼，全球招商！\\n    一座传奇的楼，见证一座城...          [周馨]  \n13  去年，上海交大成立了以杰出交大校友、中国新闻事业的杰出代表和开拓者之一的邹韬奋先生之名命名的...            []  \n14  黑皮子，就是颜色不白，有点发黑发硬的馄饨皮，最近成了热点。说是市中心有一家小店开卖这种“失传...            []  \n15  上海迪士尼度假区的各类设施和活动对天气具有高度的敏感性，在运营过程中逐渐形成了一系列“气象经...            []  \n16  昨天，该项研究成果以快速通道发表于国际肿瘤学顶级期刊《柳叶刀-肿瘤学》，影响因子达51.1分...            []  \n17  边搬设备边说不跑路\\n    住在唐镇附近的黄先生表示，两年前，恒生商业广场内开出了名为“嘟...            []  \n18  据静安区社会组织联合会会长王希佳介绍，此次援疆公益团是在静安区社会工作党委、区合作交流办、区...            []  \n19  奉贤区委书记袁泉，区委副书记、区长王益群，区人大常委会主任张培荣、区政协主席陈勇章、市慈善基...            []  \n20  上海市慈善基金会于1995年发起“放飞希望·凡人善举”手拉手结对助学项目，联合社会各界爱心人...            []  \n21  善用新功能抢票\\n    2024年春节假期从2月10日（初一）至2月17日（正月初八），共...            []  \n22  昨天东航C919上海虹桥—北京大兴航班，去程客座率超过85%，返程客座率接近100%。\\n ...            []  \n23  新图实施后，原上海虹桥站至苍南站G7537次列车，运行区段调整为上海虹桥站至吉安西站，部分运...            []  \n24  第三方调查显示：2023年，上海公众安全感和对公安工作满意度，从2013年至今已连续11年实...          [张龙]  \n25  对于一名法医来说，有3种现场是最不愿意面对的：水、火和高坠。\\n    邓德元就是一名水警法...            []  \n26  卡塔尔半岛电视台援引巴勒斯坦伊斯兰抵抗运动（哈马斯）控制的加沙卫生部门所发布消息，报道了最新...            []  \n27  布林肯8日抵达以色列，9日与以总理内塔尼亚胡、外长卡茨、国防部长加兰特举行会谈。布林肯9日在...            []  \n28  民调显示，阿塔尔是博尔内所领导内阁中最受欢迎的部长。阿塔尔相对缺乏从政经验，不过以自信、坚定...            []  \n29                                                 []            []  \n30  奥斯汀现年70岁。他于去年12月22日前往医院接受手术，并暂时将其部分权力移交给其副手、国防...            []  \n31  昨晚，在上海影城千人大厅，“宝总”（胡歌  饰）高领毛衣搭配丝绒西装，从屏幕穿越到现实。《繁...            []  \n32  早在2018年就推出第一季，至今已更新至第二季的舞台剧《繁花》，在申城舞台已迈入第七个年头。...            []  \n33  66岁的上海足坛名宿李中华，球迷们熟悉的“小头”，当年也是叱咤中国足球的著名边锋。李中华平日...            []  \n34  该剧由安徽省黄梅戏剧院创作演出，总策划为余秋雨。“梅花奖”获得者何云扮演贾宝玉，魏瑜遥饰演林...            []  \n35  本场比赛，双方延续了首回合阵容。上海女排外援布里西奥因伤缺阵，顶替她的是欧阳茜茜。队长仲慧被...            []  \n36  《七侠五义》的亮相可谓弹眼落睛。这部戏生、旦、净、丑行当齐全，唱、念、做、打样样出彩。舞台上...            []  \n37  趁着繁忙的备战期开始前，周冠宇总结过去两个赛季的得失，也袒露了对新赛季的期待。谈及此前在围场...            []  \n38  小说《繁花》与电视剧《繁花》是不完全一样的。金宇澄的长篇小说《繁花》获茅盾文学奖，受人推崇，...            []  \n39  渐梅事动，发新萼横枝，月斜香冷。枫林地外，兰雪两娥邻并。长对幽禽旧井。了无语、泉寒波定。堪怜...            []  \n40                                                 []            []  \n41  三十来年前的往事了，站在某家书店里博览群书，那时候中午出来吃碗大排面，吃完放下筷子，随便走几...            []  \n42  说来惭愧，当知青、在乡村工作13年，还没在农村过过一个完整的春节。去年春节，我到50多年前当...            []  \n43  朋友月高说：到南浔一定要喝三碗茶。\\n    一路穿街走巷，如同在锦绣年华里穿行。古镇上能做...            []  \n44  我总觉得，朋友圈是一条河流，有小溪不断汇入，也有支流或灌渠不断分岔，与干流分道扬镳。\\n  ...            []  \n45                                                 []            []  \n46  我经常光顾距离不到五百米的两家兄弟商超，它们都由母公司统一配货和管理。一家商超的冷冻食品非常...            []  \n47  《汉书·食货志》上有这样的记载：“冬，民既入；妇人同巷，相从夜绩，女工一月得四十五日。”这几...            []  \n48  邮递员通知我去门房取积压包裹和信，上楼时，邮包伴随着我的脚步声，叮当作响。到家，拆开邮包，有...            []  \n49  在上班的日子，经常需要乘飞机出差。自然而然获得各种见闻，正面的负面的都有。旅行多了，也形成独...            []  \n50  “拉猫”是一种泥塑玩具：一只小花猫，底座有一个孔洞，跟出来一根细绳，一拉绳子会发出“呜呦、呜...            []  \n51  徐悲鸿与上海的渊源极深，曾在淮海中路927弄（淮海坊）99号居住，此处是他1927年从法国留...            []  \n52  清雍正四年（1726年），奉贤设县，通津桥为华亭、奉贤的界桥。据清光绪《重修奉贤县志》记载，...            []  \n53  利好背后仍有风险挑战\\n    2024年对于汽车行业发展的有利因素包括宏观经济稳定复苏回升...            []  \n54  上海实施免费赠予新能源车牌新政后，“蓝牌”燃油小客车竞拍市场开始退烧，新能源小客车成为“香饽...            []  \n55  这其中，就有林肯汽车。\\n    如今，这个已有百年历史的豪华汽车品牌迎来再次焕新，以品牌价...            []  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_title</th>\n      <th>news_sub_title</th>\n      <th>layout_num</th>\n      <th>layout_name</th>\n      <th>date</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>authors</th>\n      <th>article</th>\n      <th>photographers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>“一江一河”滨水空间打造更多有温度的服务</td>\n      <td>带“汪汪队”逛店 与“毛孩子”友好同行</td>\n      <td>5</td>\n      <td>上海新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[杨玉红, 钱文婷]</td>\n      <td>周末带“毛孩子”去哪里撒欢，成为申城不少爱宠人士交流的热门话题。家门口的滨水公共空间成为不少...</td>\n      <td>[周馨, 钱文婷]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>“一江一河”滨水空间打造更多有温度的服务</td>\n      <td>带“汪汪队”逛店 与“毛孩子”友好同行</td>\n      <td>5</td>\n      <td>上海新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[杨玉红, 钱文婷]</td>\n      <td>周末带“毛孩子”去哪里撒欢，成为申城不少爱宠人士交流的热门话题。家门口的滨水公共空间成为不少...</td>\n      <td>[周馨, 钱文婷]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td>300余家企业已签约第七届进博会</td>\n      <td>1</td>\n      <td>一版要闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>活动现场，百事公司、美商美乐家、拜耳、特斯拉、3M、葛兰素史克等23家企业集体签约第七届进博...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>调查显示上海公众安全感和满意度连续11年“双提升”</td>\n      <td>“小目标”串起千家万户“大平安”</td>\n      <td>1</td>\n      <td>一版要闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[潘高峰]</td>\n      <td>第三方调查显示：2023年，上海公众安全感和对公安工作满意度，从2013年至今已连续11年实...</td>\n      <td>[张龙]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td></td>\n      <td>百年老大楼迎来新传奇</td>\n      <td>1</td>\n      <td>一版要闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>这里，是上海市第一面五星红旗升起地，是新中国上海市第一任市长陈毅同志的办公地。\\n    这...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>陈吉宁：更好担负起新的文化使命  奋力开创国际文化大都市建设新局面</td>\n      <td>打造习近平文化思想最佳实践地</td>\n      <td>2</td>\n      <td>要闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>李文堂的报告围绕学习习近平文化思想，理论联系实际，历史与现实相结合，从担当文化使命与文明使命...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>我国成功发射爱因斯坦探针卫星</td>\n      <td>天体爆发“捕手”启程探秘黑洞</td>\n      <td>2</td>\n      <td>要闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>爱因斯坦探针卫星由中国科学院微小卫星创新研究院抓总研制，是中国科学院空间科学先导专项继“悟空...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td></td>\n      <td>良性互动，让大学校园更好开放</td>\n      <td>2</td>\n      <td>要闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>“哈工大学霸”人手一个“橘宝宝”——来自广西南宁的11位萌娃近日在东北各城市开展“冰雪研学之...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td></td>\n      <td>崇明水仙文化旅游节举行</td>\n      <td>2</td>\n      <td>要闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>活动期间将举办花卉游园会，让传统民俗文化与新发展理念碰撞交流，推动花卉产业高质量发展，市民可...</td>\n      <td>[徐程]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td></td>\n      <td>30名上海市优秀企业家上午获表彰</td>\n      <td>2</td>\n      <td>要闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>由上海市经济和信息化委员会、上海市国有资产管理监督委员会为指导单位，上海市企业联合会、上海市...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td></td>\n      <td>让残疾人毕业生就业“无障碍”</td>\n      <td>3</td>\n      <td>综合新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>华东师大学生小蒋目前正在准备毕业论文，是一篇关于视障大学生就业现状的调查。调查发现，对于健全...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>老伯误饮曼陀罗药酒致意识不清，医生提醒——</td>\n      <td>擅用植物泡酒泡茶  当心中毒</td>\n      <td>3</td>\n      <td>综合新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>误饮一小杯药酒后中毒\\n    家住金山本地的林老伯常年腰腿疼痛不适，于是自行泡制了曼陀罗药...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>外滩将有新地标，老市府大楼更新项目3月竣工——</td>\n      <td>百年老大楼全球招商，底气何在？</td>\n      <td>4</td>\n      <td>焦点</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[姚丽萍]</td>\n      <td>外滩，将有新地标。\\n    百年老市府大楼，全球招商！\\n    一座传奇的楼，见证一座城...</td>\n      <td>[周馨]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>上海交大启动“邹韬奋卓越新闻导师领航计划”</td>\n      <td>指导培训有志学子投身新闻事业</td>\n      <td>6</td>\n      <td>上海新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>去年，上海交大成立了以杰出交大校友、中国新闻事业的杰出代表和开拓者之一的邹韬奋先生之名命名的...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td></td>\n      <td>黑皮子</td>\n      <td>6</td>\n      <td>上海新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>黑皮子，就是颜色不白，有点发黑发硬的馄饨皮，最近成了热点。说是市中心有一家小店开卖这种“失传...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>过山车、漂流要不要关？ 雨天怎么巡游？ 烟花如何燃放？</td>\n      <td>“疯狂动物城”气象服务守护游客玩乐安全</td>\n      <td>6</td>\n      <td>上海新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>上海迪士尼度假区的各类设施和活动对天气具有高度的敏感性，在运营过程中逐渐形成了一系列“气象经...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td></td>\n      <td>最毒乳腺癌靶向精准治疗获突破</td>\n      <td>6</td>\n      <td>上海新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>昨天，该项研究成果以快速通道发表于国际肿瘤学顶级期刊《柳叶刀-肿瘤学》，影响因子达51.1分...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td></td>\n      <td>“撤场前一天还在收钱充卡！”</td>\n      <td>7</td>\n      <td>帮侬忙</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>边搬设备边说不跑路\\n    住在唐镇附近的黄先生表示，两年前，恒生商业广场内开出了名为“嘟...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td></td>\n      <td>静安：社会组织联合会赴疆对口援建</td>\n      <td>7</td>\n      <td>帮侬忙</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>据静安区社会组织联合会会长王希佳介绍，此次援疆公益团是在静安区社会工作党委、区合作交流办、区...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td></td>\n      <td>奉贤：“东方美谷·风雨彩虹”圆梦行动启动</td>\n      <td>7</td>\n      <td>帮侬忙</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>奉贤区委书记袁泉，区委副书记、区长王益群，区人大常委会主任张培荣、区政协主席陈勇章、市慈善基...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>“蓝天下的至爱”爱心打卡第三阶段活动开启</td>\n      <td>“手拉手”结对助学项目邀您参与</td>\n      <td>7</td>\n      <td>帮侬忙</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>上海市慈善基金会于1995年发起“放飞希望·凡人善举”手拉手结对助学项目，联合社会各界爱心人...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td></td>\n      <td>春运火车票后天起售</td>\n      <td>8</td>\n      <td>综合新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>善用新功能抢票\\n    2024年春节假期从2月10日（初一）至2月17日（正月初八），共...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td></td>\n      <td>东航C919 首次执飞上海虹桥—北京大兴航线</td>\n      <td>8</td>\n      <td>综合新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>昨天东航C919上海虹桥—北京大兴航班，去程客座率超过85%，返程客座率接近100%。\\n ...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td></td>\n      <td>铁路上海站今起实施新列车运行图</td>\n      <td>8</td>\n      <td>综合新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>新图实施后，原上海虹桥站至苍南站G7537次列车，运行区段调整为上海虹桥站至吉安西站，部分运...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>调查显示上海公众安全感和满意度连续11年“双提升”</td>\n      <td>“小目标”串起千家万户“大平安”</td>\n      <td>8</td>\n      <td>综合新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[潘高峰]</td>\n      <td>第三方调查显示：2023年，上海公众安全感和对公安工作满意度，从2013年至今已连续11年实...</td>\n      <td>[张龙]</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td></td>\n      <td>不握手的法医</td>\n      <td>9</td>\n      <td>特稿</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[潘高峰]</td>\n      <td>对于一名法医来说，有3种现场是最不愿意面对的：水、火和高坠。\\n    邓德元就是一名水警法...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td></td>\n      <td>伤亡惨重  加沙经历“最血腥24小时”</td>\n      <td>10</td>\n      <td>国际新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>卡塔尔半岛电视台援引巴勒斯坦伊斯兰抵抗运动（哈马斯）控制的加沙卫生部门所发布消息，报道了最新...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td></td>\n      <td>美反对驱离加沙居民计划</td>\n      <td>10</td>\n      <td>国际新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>布林肯8日抵达以色列，9日与以总理内塔尼亚胡、外长卡茨、国防部长加兰特举行会谈。布林肯9日在...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>任命34岁阿塔尔担任法国新总理</td>\n      <td>马克龙选了“马克龙男孩”</td>\n      <td>10</td>\n      <td>国际新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>民调显示，阿塔尔是博尔内所领导内阁中最受欢迎的部长。阿塔尔相对缺乏从政经验，不过以自信、坚定...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td></td>\n      <td>广告</td>\n      <td>10</td>\n      <td>国际新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td></td>\n      <td>防长进了ICU  拜登三天后才知情</td>\n      <td>10</td>\n      <td>国际新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>奥斯汀现年70岁。他于去年12月22日前往医院接受手术，并暂时将其部分权力移交给其副手、国防...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>人性成为光芒，乡音成全热爱，精雕成就经典</td>\n      <td>《繁花》落尽时 何以香如故？</td>\n      <td>11</td>\n      <td>文体新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>昨晚，在上海影城千人大厅，“宝总”（胡歌  饰）高领毛衣搭配丝绒西装，从屏幕穿越到现实。《繁...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td></td>\n      <td>《繁花》舞台剧 卖到服务器崩溃</td>\n      <td>11</td>\n      <td>文体新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>早在2018年就推出第一季，至今已更新至第二季的舞台剧《繁花》，在申城舞台已迈入第七个年头。...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>上海足坛的昔日“繁花”点评“范厂长”</td>\n      <td>外表粗犷 内心细致</td>\n      <td>11</td>\n      <td>文体新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>66岁的上海足坛名宿李中华，球迷们熟悉的“小头”，当年也是叱咤中国足球的著名边锋。李中华平日...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td></td>\n      <td>经典黄梅戏赢得掌声</td>\n      <td>12</td>\n      <td>文体新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>该剧由安徽省黄梅戏剧院创作演出，总策划为余秋雨。“梅花奖”获得者何云扮演贾宝玉，魏瑜遥饰演林...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td></td>\n      <td>上海女排挺进排超决赛</td>\n      <td>12</td>\n      <td>文体新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>本场比赛，双方延续了首回合阵容。上海女排外援布里西奥因伤缺阵，顶替她的是欧阳茜茜。队长仲慧被...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td></td>\n      <td>海派京剧获得赞誉</td>\n      <td>12</td>\n      <td>文体新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>《七侠五义》的亮相可谓弹眼落睛。这部戏生、旦、净、丑行当齐全，唱、念、做、打样样出彩。舞台上...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>F1中国大奖赛昨晚开票，周冠宇现身</td>\n      <td>“在上海，目标不只是拿积分”</td>\n      <td>12</td>\n      <td>文体新闻</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>趁着繁忙的备战期开始前，周冠宇总结过去两个赛季的得失，也袒露了对新赛季的期待。谈及此前在围场...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td></td>\n      <td>从小说到电视剧</td>\n      <td>13</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>小说《繁花》与电视剧《繁花》是不完全一样的。金宇澄的长篇小说《繁花》获茅盾文学奖，受人推崇，...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td></td>\n      <td>双双燕·夜访张玉娘诗文馆有感用梅溪韵</td>\n      <td>13</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>渐梅事动，发新萼横枝，月斜香冷。枫林地外，兰雪两娥邻并。长对幽禽旧井。了无语、泉寒波定。堪怜...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td></td>\n      <td>日有吉（书法）</td>\n      <td>13</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td></td>\n      <td>关于长寿的一些思考</td>\n      <td>13</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>三十来年前的往事了，站在某家书店里博览群书，那时候中午出来吃碗大排面，吃完放下筷子，随便走几...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td></td>\n      <td>山村年味</td>\n      <td>13</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>说来惭愧，当知青、在乡村工作13年，还没在农村过过一个完整的春节。去年春节，我到50多年前当...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td></td>\n      <td>南浔三碗茶</td>\n      <td>13</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>朋友月高说：到南浔一定要喝三碗茶。\\n    一路穿街走巷，如同在锦绣年华里穿行。古镇上能做...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td></td>\n      <td>朋友圈是一条河流</td>\n      <td>14</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>我总觉得，朋友圈是一条河流，有小溪不断汇入，也有支流或灌渠不断分岔，与干流分道扬镳。\\n  ...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td></td>\n      <td>日落额尔齐斯河上</td>\n      <td>14</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td></td>\n      <td>“显而易见”很重要</td>\n      <td>14</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>我经常光顾距离不到五百米的两家兄弟商超，它们都由母公司统一配货和管理。一家商超的冷冻食品非常...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td></td>\n      <td>生命的三分之一</td>\n      <td>14</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>《汉书·食货志》上有这样的记载：“冬，民既入；妇人同巷，相从夜绩，女工一月得四十五日。”这几...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td></td>\n      <td>小摇铃</td>\n      <td>14</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>邮递员通知我去门房取积压包裹和信，上楼时，邮包伴随着我的脚步声，叮当作响。到家，拆开邮包，有...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td></td>\n      <td>乘机杂忆</td>\n      <td>14</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>在上班的日子，经常需要乘飞机出差。自然而然获得各种见闻，正面的负面的都有。旅行多了，也形成独...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td></td>\n      <td>寻“拉猫”不遇</td>\n      <td>14</td>\n      <td>夜光杯</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>“拉猫”是一种泥塑玩具：一只小花猫，底座有一个孔洞，跟出来一根细绳，一拉绳子会发出“呜呦、呜...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td></td>\n      <td>徐悲鸿与淮海坊的时间碎片</td>\n      <td>15</td>\n      <td>新民旅游</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>徐悲鸿与上海的渊源极深，曾在淮海中路927弄（淮海坊）99号居住，此处是他1927年从法国留...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td></td>\n      <td>奉贤通津桥静守时光流转</td>\n      <td>15</td>\n      <td>新民旅游</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>清雍正四年（1726年），奉贤设县，通津桥为华亭、奉贤的界桥。据清光绪《重修奉贤县志》记载，...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td></td>\n      <td>把脉2024年中国车市</td>\n      <td>16</td>\n      <td>汽车周刊/专题</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>利好背后仍有风险挑战\\n    2024年对于汽车行业发展的有利因素包括宏观经济稳定复苏回升...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td></td>\n      <td>上海私家车牌竞拍中标率大升</td>\n      <td>16</td>\n      <td>汽车周刊/专题</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>上海实施免费赠予新能源车牌新政后，“蓝牌”燃油小客车竞拍市场开始退烧，新能源小客车成为“香饽...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td></td>\n      <td>深耕价值营销  林肯在华迎来50万名用户</td>\n      <td>16</td>\n      <td>汽车周刊/专题</td>\n      <td>2024-01-10</td>\n      <td>2024</td>\n      <td>01</td>\n      <td>10</td>\n      <td>[]</td>\n      <td>这其中，就有林肯汽车。\\n    如今，这个已有百年历史的豪华汽车品牌迎来再次焕新，以品牌价...</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_links = news_content_soup.find_all('a', href=re.compile('/html/xmwb/'))\n",
    "other_page_urls = []\n",
    "for link in other_links:\n",
    "    rel_url = link.get('href')\n",
    "    other_page_urls.append(rel_url)\n",
    "url_lst = []\n",
    "df = pandas.DataFrame({\n",
    "            'news_title' : [],\n",
    "            'news_sub_title' : [],\n",
    "            'layout_num' : [],\n",
    "            'layout_name' : [],\n",
    "            'date' : [],\n",
    "            'year' : [],\n",
    "            'month' : [],\n",
    "            'day' : [],\n",
    "            'authors' : [],\n",
    "            'article' : [],\n",
    "            'photographers' : []})\n",
    "def add_info_to_df(df, start_url, url_lst):\n",
    "    if start_url not in url_lst or df.empty or len(df) < 10:\n",
    "        url_lst.append(start_url)\n",
    "        news_web_request = requests.get(start_url)\n",
    "        news_web_request.encoding = 'utf-8'\n",
    "        news_content_soup = bs4.BeautifulSoup(news_web_request.text, 'html.parser')\n",
    "        report_info = news_content_soup.find_all('p', class_ = 'dzb-desc-box')\n",
    "        combined_text = ' '.join([report.get_text() for report in report_info])\n",
    "        select_pattern = re.compile(r'>>>')\n",
    "        select_match = re.search(select_pattern, combined_text)\n",
    "        if not select_match:\n",
    "            news_title = news_content_soup.find_all('span', class_ = 'dzb-special-title-box')\n",
    "            news_title_lst = [span.get_text() for span in news_title]\n",
    "            news_sub_title = news_content_soup.find_all('h2', class_ = 'dzb-title-box')\n",
    "            news_sub_title_lst = [span.get_text() for span in news_sub_title]\n",
    "            layout_info = news_content_soup.find_all('span', class_ = 'dzb-banmian-title')\n",
    "            layout_text = [span.get_text() for span in layout_info]\n",
    "            layout_num_pattern = re.compile(r'第(\\d+)版')\n",
    "            layout_name_pattern = re.compile(r'：(.*)')\n",
    "            layout_num = [re.search(layout_num_pattern, layout_text[0]).group(1)]\n",
    "            layout_name = [re.search(layout_name_pattern, layout_text[0]).group(1)]\n",
    "            date_info = news_content_soup.find_all('span', class_ = 'dzb-banmian-date')\n",
    "            date_text = [span.get_text() for span in date_info]\n",
    "            date_pattern = re.compile(r'((\\d{4})-(\\d{2})-(\\d{2}))')\n",
    "            date_match = re.search(date_pattern, date_text[0])\n",
    "            date = [date_match.group(1)]\n",
    "            year = [date_match.group(2)]\n",
    "            month = [date_match.group(3)]\n",
    "            day = [date_match.group(4)]\n",
    "            #report_info = news_content_soup.find_all('p', class_ = 'dzb-desc-box')\n",
    "            report_text = report_info[0].get_text(separator = '\\n')\n",
    "            authors_article = report_text.split('\\n', 1)\n",
    "            authors_text = authors_article[0]\n",
    "            if len(authors_article) >= 2:\n",
    "                report_text = authors_article[1].strip()\n",
    "            else:\n",
    "                report_text = []\n",
    "            authors_pattern = r'本报记者\\s+([\\u4e00-\\u9fa5\\s]+)'\n",
    "            authors_match = re.search(authors_pattern, authors_text)\n",
    "            if authors_match:\n",
    "                authors = [authors_match.group(1).split()]\n",
    "            else:\n",
    "                authors = [[]]\n",
    "            article = [report_text]\n",
    "            photograph_info = news_content_soup.find_all('p', style=\"padding: 3px;white-space: pre-line;word-break: normal;\")\n",
    "            photograph_info_lst = [p.get_text() for p in photograph_info]\n",
    "            photographer_pattern = r'本报记者\\s+([\\u4e00-\\u9fa5]+)\\s+摄'\n",
    "            photographer_lst = []\n",
    "            for text in photograph_info_lst:\n",
    "                photographer_match = re.search(photographer_pattern, text)\n",
    "                if photographer_match:\n",
    "                    if photographer_match.group(1) not in photographer_lst:\n",
    "                        photographer_lst.append(photographer_match.group(1))\n",
    "            photographer_lst = [photographer_lst]\n",
    "            news_info_df = pandas.DataFrame({\n",
    "            'news_title' : news_title_lst,\n",
    "            'news_sub_title' : news_sub_title_lst,\n",
    "            'layout_num' : layout_num,\n",
    "            'layout_name' : layout_name,\n",
    "            'date' : date,\n",
    "            'year' : year,\n",
    "            'month' : month,\n",
    "            'day' : day,\n",
    "            'authors' : authors,\n",
    "            'article' : article,\n",
    "            'photographers' : photographer_lst})\n",
    "            df = pd.concat([df, news_info_df],ignore_index=True)\n",
    "            other_links = news_content_soup.find_all('a', href=re.compile('/html/xmwb/'))\n",
    "            for link in other_links:\n",
    "                rel_url = 'https://paper.xinmin.cn/' + link.get('href')\n",
    "                if rel_url not in url_lst:\n",
    "                    df = add_info_to_df(df, rel_url, url_lst)\n",
    "    return df\n",
    "df = add_info_to_df(df, news_web, url_lst)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T05:22:00.880189Z",
     "start_time": "2024-01-11T05:20:55.003226Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API (Tumblr)\n",
    "\n",
    "Generally website owners do not like you scraping their sites. If done badly,\n",
    "scarping can act like a DOS attack so you should be careful how often you make\n",
    "calls to a site. Some sites want automated tools to access their data, so they\n",
    "create [application programming interface\n",
    "(APIs)](https://en.wikipedia.org/wiki/Application_programming_interface). An API\n",
    "specifies a procedure for an application (or script) to access their data. Often\n",
    "this is though a [representational state transfer\n",
    "(REST)](https://en.wikipedia.org/wiki/Representational_state_transfer) web\n",
    "service, which just means if you make correctly formatted HTTP requests they\n",
    "will return nicely formatted data.\n",
    "\n",
    "A nice example for us to study is [Tumblr](https://www.tumblr.com), they have a\n",
    "[simple RESTful API](https://www.tumblr.com/docs/en/api/v1) that allows you to\n",
    "read posts without any complicated html parsing.\n",
    "\n",
    "We can get the first 20 posts from a blog by making an http GET request to\n",
    "`'http://{blog}.tumblr.com/api/read/json'`, were `{blog}` is the name of the\n",
    "target blog. Lets try and get the posts from [http://lolcats-lol-\n",
    "cat.tumblr.com/](http://lolcats-lol-cat.tumblr.com/) (Note the blog says at the\n",
    "top 'One hour one pic lolcats', but the canonical name that Tumblr uses is in\n",
    "the URL 'lolcats-lol-cat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:22:01.228326Z",
     "start_time": "2024-01-11T05:22:00.879506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var tumblr_api_read = {\"tumblelog\":{\"title\":\"One hour one pic lolcats\",\"description\":\"\",\"name\":\"lolcats-lol-cat\",\"timezone\":\"Europe\\/Paris\",\"cname\":false,\"feeds\":[],\"uuid\":\"t:nXFqyQsaizVnIxVAm-ttlA\"},\"posts-start\":0,\"posts-total\":3926,\"posts-type\":false,\"posts\":[{\"id\":\"679413944568430592\",\"url\":\"https:\\/\\/lolcats-lol-cat.tumblr.com\\/post\\/679413944568430592\",\"url-with-slug\":\"https:\\/\\/lolcats-lol-cat.tumblr.com\\/post\\/679413944568430592\\/cat-cats-kitty-gato-saturday-meow-katze\",\"type\":\"photo\",\"date-gmt\":\"2022-03-22 09:00:29 GMT\",\"date\":\"Tue, 22 Mar 2022 10:00:29\",\"bookmarklet\":0,\"mobile\":0,\"feed-item\":\"\",\"from-feed-id\":0,\"unix-timestamp\":1647939629,\"format\":\"html\",\"reblog-key\":\"MDkw6o5C\",\"slug\":\"cat-cats-kitty-gato-saturday-meow-katze\",\"is-submission\":false,\"like-button\":\"<div class=\\\"like_button\\\" data-post-id=\\\"679413944568430592\\\" data-blog-name=\\\"lolcats-lol-cat\\\" id=\\\"like_button_679413944568430592\\\"><iframe id=\\\"like_iframe_679413944568430592\\\" src=\\\"https:\\/\\/assets.tumblr.com\\/\n"
     ]
    }
   ],
   "source": [
    "tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'))\n",
    "\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T04:59:16.662601Z",
     "start_time": "2024-01-10T04:59:16.661319Z"
    }
   },
   "source": [
    "This might not look very good on first inspection, but it has far fewer angle\n",
    "braces than html, which makes it easier to parse. What we have is\n",
    "[JSON](https://en.wikipedia.org/wiki/JSON) a 'human readable' text based data\n",
    "transmission format based on javascript. Luckily, we can readily convert it to a\n",
    "python `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:22:01.233055Z",
     "start_time": "2024-01-11T05:22:01.229123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tumblelog', 'posts-start', 'posts-total', 'posts-type', 'posts'])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#We need to load only the stuff between the curly braces\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "print(d.keys())\n",
    "print(len(d['posts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.662848Z"
    }
   },
   "source": [
    "If we read the [API specification](https://www.tumblr.com/docs/en/api/v1), we\n",
    "will see there are a lot of things we can get if we add things to our GET\n",
    "request. First we can retrieve posts by their id number. Let's first get post\n",
    "`146020177084`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:22:01.563402Z",
     "start_time": "2024-01-11T05:22:01.233239Z"
    }
   },
   "outputs": [],
   "source": [
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'), params = {'id' : 146020177084})\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "d['posts'][0].keys()\n",
    "d['posts'][0]['photo-url-1280']\n",
    "\n",
    "with open('lolcat.gif', 'wb') as f:\n",
    "    gifRequest = requests.get(d['posts'][0]['photo-url-1280'], stream = True)\n",
    "    f.write(gifRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T04:59:16.664471Z",
     "start_time": "2024-01-10T04:59:16.663882Z"
    }
   },
   "source": [
    "<img src='lolcat.gif'>\n",
    "\n",
    "Such beauty; such vigor (If you can't see it you have to refresh the page). Now\n",
    "we could retrieve the text from all posts as well\n",
    "as related metadata, like the post date, caption or tags. We could also get\n",
    "links to all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:22:02.016875Z",
     "start_time": "2024-01-11T05:22:01.567809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    id                                          photo-url  \\\n0   679413944568430592  https://64.media.tumblr.com/1fb7e346f39e428540...   \n1   662815854023655425  https://64.media.tumblr.com/021eac8fbcafbb00a5...   \n2   662778109891952640  https://64.media.tumblr.com/8c0517adb8c71e4a3d...   \n3   662657302700146688  https://64.media.tumblr.com/061d27cda309d5c809...   \n4   662513901538246656  https://64.media.tumblr.com/80584a9d1ff4ddc4fc...   \n5   662257177983090688  https://64.media.tumblr.com/893b320cd2e8970a20...   \n6   662166591527698432  https://64.media.tumblr.com/c7f0a0a9184e480e15...   \n7   662113740899090432  https://64.media.tumblr.com/07f7be7f71917a6049...   \n8   661955166248026112  https://64.media.tumblr.com/205f030c48d31f8960...   \n9   661894830378614784  https://64.media.tumblr.com/c463bff883fec2045b...   \n10  661864596682964992  https://64.media.tumblr.com/b840ddb3aa2303206b...   \n11  661826822696321024  https://64.media.tumblr.com/adc18e35b1844be41a...   \n12  661804260632215552  https://64.media.tumblr.com/caae5b433ada6d6cb3...   \n13  661766453508407296  https://64.media.tumblr.com/9d8cf92208aec32b93...   \n14  661623017704685568  https://64.media.tumblr.com/4a5ae6732ab54227a4...   \n15  661555057316298752  https://64.media.tumblr.com/c92a31829a6d88381e...   \n16  661509804401065984  https://64.media.tumblr.com/2653be7b6e3a8efb9e...   \n17  661411625970106368  https://64.media.tumblr.com/e508b4d0fcc083750b...   \n18  661260621178945536  https://64.media.tumblr.com/28e64b8d19014a4c98...   \n19  661177569862991872  https://64.media.tumblr.com/9ffadbe3e1d5fb20f8...   \n20  661154964805140480  https://64.media.tumblr.com/27dce173c9e6cf3f05...   \n21  660988818837651457  https://64.media.tumblr.com/02db958e8649c04d19...   \n22  660935942249578496  https://64.media.tumblr.com/80b5d0f3c5636990b9...   \n23  660905765646843905  https://64.media.tumblr.com/a2eb4c91986a0c1e23...   \n24  660898202755661824  https://64.media.tumblr.com/061d27cda309d5c809...   \n25  660883122996576256  https://64.media.tumblr.com/2d1137cbf0dbe21d8b...   \n26  660732105459924992  https://64.media.tumblr.com/cccbbc09b009b581a9...   \n27  660475424163364864  https://64.media.tumblr.com/9d700e79b1615fac2f...   \n28  660324431590277120  https://64.media.tumblr.com/7cc2c0f675bc099be4...   \n29  660196106964221952  https://64.media.tumblr.com/c024195387a46a6f77...   \n30  660143257603506176  https://64.media.tumblr.com/fdf46563a9b09ec147...   \n31  660060193333346304  https://64.media.tumblr.com/4183c34ae67f92f290...   \n32  659743087466332160  https://64.media.tumblr.com/01c63fcc1d5fc2e0e0...   \n33  659705393411391488  https://64.media.tumblr.com/42a1477d18728fb088...   \n34  659660051218317312  https://64.media.tumblr.com/a4392b133ed186fad8...   \n35  659539266591981568  https://64.media.tumblr.com/9024bc51ad17d40b80...   \n36  659509078641557504  https://64.media.tumblr.com/12afb1b79928488a1a...   \n37  659342999892967424  https://64.media.tumblr.com/29a90b59498d9ae55f...   \n38  659108907014356992  https://64.media.tumblr.com/7a11be8b9524535da2...   \n39  658867320238850049  https://64.media.tumblr.com/bf7efc7edad65675ad...   \n40  658844675622453248  https://64.media.tumblr.com/c2b58415fe3fed86a2...   \n41  658814510935769088  https://64.media.tumblr.com/d70155b2bdd5763cca...   \n42  658338845753999360  https://64.media.tumblr.com/1fca341b03f832d679...   \n43  658225598089609217  https://64.media.tumblr.com/07aad399cc881a2399...   \n44  658210575863742464  https://64.media.tumblr.com/d5b0639155ee97c854...   \n45  658165190497583104  https://64.media.tumblr.com/ea6ad0eae613c5f60f...   \n46  657946256220536832  https://64.media.tumblr.com/16cf3a5e394112f7d0...   \n47  657931173576704000  https://64.media.tumblr.com/2b7dfb6b1677079a45...   \n48  657312093673078784  https://64.media.tumblr.com/17fe9380eed8516e45...   \n49  656677912834195456  https://64.media.tumblr.com/7c4e668dc5eb0e89ed...   \n\n                         date  \\\n0   Tue, 22 Mar 2022 10:00:29   \n1   Mon, 20 Sep 2021 06:00:56   \n2   Sun, 19 Sep 2021 20:01:00   \n3   Sat, 18 Sep 2021 12:00:50   \n4   Thu, 16 Sep 2021 22:01:32   \n5   Tue, 14 Sep 2021 02:01:01   \n6   Mon, 13 Sep 2021 02:01:11   \n7   Sun, 12 Sep 2021 12:01:09   \n8   Fri, 10 Sep 2021 18:00:40   \n9   Fri, 10 Sep 2021 02:01:39   \n10  Thu, 09 Sep 2021 18:01:06   \n11  Thu, 09 Sep 2021 08:00:42   \n12  Thu, 09 Sep 2021 02:02:05   \n13  Wed, 08 Sep 2021 16:01:10   \n14  Tue, 07 Sep 2021 02:01:19   \n15  Mon, 06 Sep 2021 08:01:07   \n16  Sun, 05 Sep 2021 20:01:50   \n17  Sat, 04 Sep 2021 18:01:20   \n18  Fri, 03 Sep 2021 02:01:10   \n19  Thu, 02 Sep 2021 04:01:06   \n20  Wed, 01 Sep 2021 22:01:49   \n21  Tue, 31 Aug 2021 02:00:59   \n22  Mon, 30 Aug 2021 12:00:32   \n23  Mon, 30 Aug 2021 04:00:54   \n24  Mon, 30 Aug 2021 02:00:41   \n25  Sun, 29 Aug 2021 22:01:00   \n26  Sat, 28 Aug 2021 06:00:38   \n27  Wed, 25 Aug 2021 10:00:48   \n28  Mon, 23 Aug 2021 18:00:50   \n29  Sun, 22 Aug 2021 08:01:10   \n30  Sat, 21 Aug 2021 18:01:09   \n31  Fri, 20 Aug 2021 20:00:53   \n32  Tue, 17 Aug 2021 08:00:37   \n33  Mon, 16 Aug 2021 22:01:30   \n34  Mon, 16 Aug 2021 10:00:48   \n35  Sun, 15 Aug 2021 02:00:59   \n36  Sat, 14 Aug 2021 18:01:09   \n37  Thu, 12 Aug 2021 22:01:24   \n38  Tue, 10 Aug 2021 08:00:36   \n39  Sat, 07 Aug 2021 16:00:41   \n40  Sat, 07 Aug 2021 10:00:45   \n41  Sat, 07 Aug 2021 02:01:18   \n42  Sun, 01 Aug 2021 20:00:48   \n43  Sat, 31 Jul 2021 14:00:47   \n44  Sat, 31 Jul 2021 10:02:00   \n45  Fri, 30 Jul 2021 22:00:38   \n46  Wed, 28 Jul 2021 12:00:46   \n47  Wed, 28 Jul 2021 08:01:02   \n48  Wed, 21 Jul 2021 12:01:01   \n49  Wed, 14 Jul 2021 12:00:59   \n\n                                                 tags photo-type  \n0   [gif, lolcat, lolcats, cat, funny, cats, kitty...        gif  \n1                  [gif, lolcat, lolcats, cat, funny]        gif  \n2                   [cat, cats, lol, lolcat, lolcats]        png  \n3                   [cat, cats, lol, lolcat, lolcats]        jpg  \n4                   [cat, cats, lol, lolcat, lolcats]        png  \n5                   [cat, cats, lol, lolcat, lolcats]        jpg  \n6                   [cat, cats, lol, lolcat, lolcats]        jpg  \n7                   [cat, cats, lol, lolcat, lolcats]        jpg  \n8                   [cat, cats, lol, lolcat, lolcats]        jpg  \n9                   [cat, cats, lol, lolcat, lolcats]        png  \n10                  [cat, cats, lol, lolcat, lolcats]        jpg  \n11                  [cat, cats, lol, lolcat, lolcats]        png  \n12                  [cat, cats, lol, lolcat, lolcats]        png  \n13                  [cat, cats, lol, lolcat, lolcats]        jpg  \n14                  [cat, cats, lol, lolcat, lolcats]        jpg  \n15                  [cat, cats, lol, lolcat, lolcats]        jpg  \n16                  [cat, cats, lol, lolcat, lolcats]        png  \n17                  [cat, cats, lol, lolcat, lolcats]        jpg  \n18                  [cat, cats, lol, lolcat, lolcats]        png  \n19                  [cat, cats, lol, lolcat, lolcats]        png  \n20                  [cat, cats, lol, lolcat, lolcats]        jpg  \n21                  [cat, cats, lol, lolcat, lolcats]        jpg  \n22                  [cat, cats, lol, lolcat, lolcats]        jpg  \n23                  [cat, cats, lol, lolcat, lolcats]        jpg  \n24                  [cat, cats, lol, lolcat, lolcats]        jpg  \n25                  [cat, cats, lol, lolcat, lolcats]        png  \n26                  [cat, cats, lol, lolcat, lolcats]        jpg  \n27                  [cat, cats, lol, lolcat, lolcats]        jpg  \n28                  [cat, cats, lol, lolcat, lolcats]        png  \n29                  [cat, cats, lol, lolcat, lolcats]        png  \n30                  [cat, cats, lol, lolcat, lolcats]        jpg  \n31                  [cat, cats, lol, lolcat, lolcats]        jpg  \n32                  [cat, cats, lol, lolcat, lolcats]        png  \n33                  [cat, cats, lol, lolcat, lolcats]        png  \n34                  [cat, cats, lol, lolcat, lolcats]        jpg  \n35                  [cat, cats, lol, lolcat, lolcats]        jpg  \n36                  [cat, cats, lol, lolcat, lolcats]        png  \n37                  [cat, cats, lol, lolcat, lolcats]        jpg  \n38                  [cat, cats, lol, lolcat, lolcats]        jpg  \n39                  [cat, cats, lol, lolcat, lolcats]        jpg  \n40                  [cat, cats, lol, lolcat, lolcats]        jpg  \n41                  [cat, cats, lol, lolcat, lolcats]        jpg  \n42                  [cat, cats, lol, lolcat, lolcats]        png  \n43                  [cat, cats, lol, lolcat, lolcats]        jpg  \n44                  [cat, cats, lol, lolcat, lolcats]        jpg  \n45                 [gif, lolcat, lolcats, cat, funny]        gif  \n46                  [cat, cats, lol, lolcat, lolcats]        jpg  \n47                  [cat, cats, lol, lolcat, lolcats]        jpg  \n48                  [cat, cats, lol, lolcat, lolcats]        png  \n49                  [cat, cats, lol, lolcat, lolcats]        jpg  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>photo-url</th>\n      <th>date</th>\n      <th>tags</th>\n      <th>photo-type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>679413944568430592</td>\n      <td>https://64.media.tumblr.com/1fb7e346f39e428540...</td>\n      <td>Tue, 22 Mar 2022 10:00:29</td>\n      <td>[gif, lolcat, lolcats, cat, funny, cats, kitty...</td>\n      <td>gif</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>662815854023655425</td>\n      <td>https://64.media.tumblr.com/021eac8fbcafbb00a5...</td>\n      <td>Mon, 20 Sep 2021 06:00:56</td>\n      <td>[gif, lolcat, lolcats, cat, funny]</td>\n      <td>gif</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>662778109891952640</td>\n      <td>https://64.media.tumblr.com/8c0517adb8c71e4a3d...</td>\n      <td>Sun, 19 Sep 2021 20:01:00</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>662657302700146688</td>\n      <td>https://64.media.tumblr.com/061d27cda309d5c809...</td>\n      <td>Sat, 18 Sep 2021 12:00:50</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>662513901538246656</td>\n      <td>https://64.media.tumblr.com/80584a9d1ff4ddc4fc...</td>\n      <td>Thu, 16 Sep 2021 22:01:32</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>662257177983090688</td>\n      <td>https://64.media.tumblr.com/893b320cd2e8970a20...</td>\n      <td>Tue, 14 Sep 2021 02:01:01</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>662166591527698432</td>\n      <td>https://64.media.tumblr.com/c7f0a0a9184e480e15...</td>\n      <td>Mon, 13 Sep 2021 02:01:11</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>662113740899090432</td>\n      <td>https://64.media.tumblr.com/07f7be7f71917a6049...</td>\n      <td>Sun, 12 Sep 2021 12:01:09</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>661955166248026112</td>\n      <td>https://64.media.tumblr.com/205f030c48d31f8960...</td>\n      <td>Fri, 10 Sep 2021 18:00:40</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>661894830378614784</td>\n      <td>https://64.media.tumblr.com/c463bff883fec2045b...</td>\n      <td>Fri, 10 Sep 2021 02:01:39</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>661864596682964992</td>\n      <td>https://64.media.tumblr.com/b840ddb3aa2303206b...</td>\n      <td>Thu, 09 Sep 2021 18:01:06</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>661826822696321024</td>\n      <td>https://64.media.tumblr.com/adc18e35b1844be41a...</td>\n      <td>Thu, 09 Sep 2021 08:00:42</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>661804260632215552</td>\n      <td>https://64.media.tumblr.com/caae5b433ada6d6cb3...</td>\n      <td>Thu, 09 Sep 2021 02:02:05</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>661766453508407296</td>\n      <td>https://64.media.tumblr.com/9d8cf92208aec32b93...</td>\n      <td>Wed, 08 Sep 2021 16:01:10</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>661623017704685568</td>\n      <td>https://64.media.tumblr.com/4a5ae6732ab54227a4...</td>\n      <td>Tue, 07 Sep 2021 02:01:19</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>661555057316298752</td>\n      <td>https://64.media.tumblr.com/c92a31829a6d88381e...</td>\n      <td>Mon, 06 Sep 2021 08:01:07</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>661509804401065984</td>\n      <td>https://64.media.tumblr.com/2653be7b6e3a8efb9e...</td>\n      <td>Sun, 05 Sep 2021 20:01:50</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>661411625970106368</td>\n      <td>https://64.media.tumblr.com/e508b4d0fcc083750b...</td>\n      <td>Sat, 04 Sep 2021 18:01:20</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>661260621178945536</td>\n      <td>https://64.media.tumblr.com/28e64b8d19014a4c98...</td>\n      <td>Fri, 03 Sep 2021 02:01:10</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>661177569862991872</td>\n      <td>https://64.media.tumblr.com/9ffadbe3e1d5fb20f8...</td>\n      <td>Thu, 02 Sep 2021 04:01:06</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>661154964805140480</td>\n      <td>https://64.media.tumblr.com/27dce173c9e6cf3f05...</td>\n      <td>Wed, 01 Sep 2021 22:01:49</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>660988818837651457</td>\n      <td>https://64.media.tumblr.com/02db958e8649c04d19...</td>\n      <td>Tue, 31 Aug 2021 02:00:59</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>660935942249578496</td>\n      <td>https://64.media.tumblr.com/80b5d0f3c5636990b9...</td>\n      <td>Mon, 30 Aug 2021 12:00:32</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>660905765646843905</td>\n      <td>https://64.media.tumblr.com/a2eb4c91986a0c1e23...</td>\n      <td>Mon, 30 Aug 2021 04:00:54</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>660898202755661824</td>\n      <td>https://64.media.tumblr.com/061d27cda309d5c809...</td>\n      <td>Mon, 30 Aug 2021 02:00:41</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>660883122996576256</td>\n      <td>https://64.media.tumblr.com/2d1137cbf0dbe21d8b...</td>\n      <td>Sun, 29 Aug 2021 22:01:00</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>660732105459924992</td>\n      <td>https://64.media.tumblr.com/cccbbc09b009b581a9...</td>\n      <td>Sat, 28 Aug 2021 06:00:38</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>660475424163364864</td>\n      <td>https://64.media.tumblr.com/9d700e79b1615fac2f...</td>\n      <td>Wed, 25 Aug 2021 10:00:48</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>660324431590277120</td>\n      <td>https://64.media.tumblr.com/7cc2c0f675bc099be4...</td>\n      <td>Mon, 23 Aug 2021 18:00:50</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>660196106964221952</td>\n      <td>https://64.media.tumblr.com/c024195387a46a6f77...</td>\n      <td>Sun, 22 Aug 2021 08:01:10</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>660143257603506176</td>\n      <td>https://64.media.tumblr.com/fdf46563a9b09ec147...</td>\n      <td>Sat, 21 Aug 2021 18:01:09</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>660060193333346304</td>\n      <td>https://64.media.tumblr.com/4183c34ae67f92f290...</td>\n      <td>Fri, 20 Aug 2021 20:00:53</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>659743087466332160</td>\n      <td>https://64.media.tumblr.com/01c63fcc1d5fc2e0e0...</td>\n      <td>Tue, 17 Aug 2021 08:00:37</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>659705393411391488</td>\n      <td>https://64.media.tumblr.com/42a1477d18728fb088...</td>\n      <td>Mon, 16 Aug 2021 22:01:30</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>659660051218317312</td>\n      <td>https://64.media.tumblr.com/a4392b133ed186fad8...</td>\n      <td>Mon, 16 Aug 2021 10:00:48</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>659539266591981568</td>\n      <td>https://64.media.tumblr.com/9024bc51ad17d40b80...</td>\n      <td>Sun, 15 Aug 2021 02:00:59</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>659509078641557504</td>\n      <td>https://64.media.tumblr.com/12afb1b79928488a1a...</td>\n      <td>Sat, 14 Aug 2021 18:01:09</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>659342999892967424</td>\n      <td>https://64.media.tumblr.com/29a90b59498d9ae55f...</td>\n      <td>Thu, 12 Aug 2021 22:01:24</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>659108907014356992</td>\n      <td>https://64.media.tumblr.com/7a11be8b9524535da2...</td>\n      <td>Tue, 10 Aug 2021 08:00:36</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>658867320238850049</td>\n      <td>https://64.media.tumblr.com/bf7efc7edad65675ad...</td>\n      <td>Sat, 07 Aug 2021 16:00:41</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>658844675622453248</td>\n      <td>https://64.media.tumblr.com/c2b58415fe3fed86a2...</td>\n      <td>Sat, 07 Aug 2021 10:00:45</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>658814510935769088</td>\n      <td>https://64.media.tumblr.com/d70155b2bdd5763cca...</td>\n      <td>Sat, 07 Aug 2021 02:01:18</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>658338845753999360</td>\n      <td>https://64.media.tumblr.com/1fca341b03f832d679...</td>\n      <td>Sun, 01 Aug 2021 20:00:48</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>658225598089609217</td>\n      <td>https://64.media.tumblr.com/07aad399cc881a2399...</td>\n      <td>Sat, 31 Jul 2021 14:00:47</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>658210575863742464</td>\n      <td>https://64.media.tumblr.com/d5b0639155ee97c854...</td>\n      <td>Sat, 31 Jul 2021 10:02:00</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>658165190497583104</td>\n      <td>https://64.media.tumblr.com/ea6ad0eae613c5f60f...</td>\n      <td>Fri, 30 Jul 2021 22:00:38</td>\n      <td>[gif, lolcat, lolcats, cat, funny]</td>\n      <td>gif</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>657946256220536832</td>\n      <td>https://64.media.tumblr.com/16cf3a5e394112f7d0...</td>\n      <td>Wed, 28 Jul 2021 12:00:46</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>657931173576704000</td>\n      <td>https://64.media.tumblr.com/2b7dfb6b1677079a45...</td>\n      <td>Wed, 28 Jul 2021 08:01:02</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>657312093673078784</td>\n      <td>https://64.media.tumblr.com/17fe9380eed8516e45...</td>\n      <td>Wed, 21 Jul 2021 12:01:01</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>png</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>656677912834195456</td>\n      <td>https://64.media.tumblr.com/7c4e668dc5eb0e89ed...</td>\n      <td>Wed, 14 Jul 2021 12:00:59</td>\n      <td>[cat, cats, lol, lolcat, lolcats]</td>\n      <td>jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Putting a max in case the blog has millions of images\n",
    "#The given max will be rounded up to the nearest multiple of 50\n",
    "def tumblrImageScrape(blogName, maxImages = 200):\n",
    "    #Restating this here so the function isn't dependent on any external variables\n",
    "    tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "    #There are a bunch of possible locations for the photo url\n",
    "    possiblePhotoSuffixes = [1280, 500, 400, 250, 100]\n",
    "\n",
    "    #These are the pieces of information we will be gathering,\n",
    "    #at the end we will convert this to a DataFrame.\n",
    "    #There are a few other datums we could gather like the captions\n",
    "    #you can read the Tumblr documentation to learn how to get them\n",
    "    #https://www.tumblr.com/docs/en/api/v1\n",
    "    postsData = {\n",
    "        'id' : [],\n",
    "        'photo-url' : [],\n",
    "        'date' : [],\n",
    "        'tags' : [],\n",
    "        'photo-type' : []\n",
    "    }\n",
    "\n",
    "    #Tumblr limits us to a max of 50 posts per request\n",
    "    for requestNum in range(maxImages // 50):\n",
    "        requestParams = {\n",
    "            'start' : requestNum * 50,\n",
    "            'num' : 50,\n",
    "            'type' : 'photo'\n",
    "        }\n",
    "        r = requests.get(tumblrAPItarget.format(blogName), params = requestParams)\n",
    "        requestDict = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "        for postDict in requestDict['posts']:\n",
    "            #We are dealing with uncleaned data, we can't trust it.\n",
    "            #Specifically, not all posts are guaranteed to have the fields we want\n",
    "            try:\n",
    "                postsData['id'].append(postDict['id'])\n",
    "                postsData['date'].append(postDict['date'])\n",
    "                postsData['tags'].append(postDict['tags'])\n",
    "            except KeyError as e:\n",
    "                raise KeyError(\"Post {} from {} is missing: {}\".format(postDict['id'], blogName, e))\n",
    "\n",
    "            foundSuffix = False\n",
    "            for suffix in possiblePhotoSuffixes:\n",
    "                try:\n",
    "                    photoURL = postDict['photo-url-{}'.format(suffix)]\n",
    "                    postsData['photo-url'].append(photoURL)\n",
    "                    postsData['photo-type'].append(photoURL.split('.')[-1])\n",
    "                    foundSuffix = True\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if not foundSuffix:\n",
    "                #Make sure your error messages are useful\n",
    "                #You will be one of the users\n",
    "                raise KeyError(\"Post {} from {} is missing a photo url\".format(postDict['id'], blogName))\n",
    "\n",
    "    return pandas.DataFrame(postsData)\n",
    "tumblrImageScrape('lolcats-lol-cat', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.665168Z"
    }
   },
   "source": [
    "Now we have the urls of a bunch of images and can run OCR on them to gather\n",
    "compelling meme narratives, accompanied by cats.\n",
    "\n",
    "# Files\n",
    "\n",
    "What if the text we want isn't on a webpage? There are a many other sources of\n",
    "text available, typically organized into *files*.\n",
    "\n",
    "## Raw text (and encoding)\n",
    "\n",
    "The most basic form of storing text is as a _raw text_ document. Source code\n",
    "(`.py`, `.r`, etc) is usually raw text as are text files (`.txt`) and those with\n",
    "many other extension (e.g., .csv, .dat, etc.). Opening an unknown file with a\n",
    "text editor is often a great way of learning what the file is.\n",
    "\n",
    "We can create a text file in python with the `open()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:22:02.026311Z",
     "start_time": "2024-01-11T05:22:02.016485Z"
    }
   },
   "outputs": [],
   "source": [
    "#example_text_file = 'sometextfile.txt'\n",
    "#stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols \\u2421 \\u241B \\u20A0 \\u20A1 \\u20A2 \\u20A3 \\u0D60\\n'\n",
    "stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\\n'\n",
    "\n",
    "with open(example_text_file, mode = 'w', encoding='utf-8') as f:\n",
    "    f.write(stringToWrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.666520Z"
    }
   },
   "source": [
    "Notice the `encoding='utf-8'` argument, which specifies how we map the bits from\n",
    "the file to the glyphs (and whitespace characters like tab (`'\\t'`) or newline\n",
    "(`'\\n'`)) on the screen. When dealing only with latin letters, arabic numerals\n",
    "and the other symbols on America keyboards you usually do not have to worry\n",
    "about encodings as the ones used today are backwards compatible with\n",
    "[ASCII](https://en.wikipedia.org/wiki/ASCII), which gives the binary\n",
    "representation of 128 characters.\n",
    "\n",
    "Some of you, however, will want to use other characters (e.g., Chinese\n",
    "characters). To solve this there is\n",
    "[Unicode](https://en.wikipedia.org/wiki/Unicode) which assigns numbers to\n",
    "symbols, e.g., 041 is `'A'` and 03A3 is `'Σ'` (numbers starting with 0 are\n",
    "hexadecimal). Often non/beyond-ASCII characters are called Unicode characters.\n",
    "Unicode contains 1,114,112 characters, about 10\\% of which have been assigned.\n",
    "Unfortunately there are many ways used to map combinations of bits to Unicode\n",
    "symbols. The ones you are likely to encounter are called by Python _utf-8_,\n",
    "_utf-16_ and _latin-1_. _utf-8_ is the standard for Linux and Mac OS while both\n",
    "_utf-16_ and _latin-1_ are used by windows. If you use the wrong encoding,\n",
    "characters can appear wrong, sometimes change in number or Python could raise an\n",
    "exception. Lets see what happens when we open the file we just created with\n",
    "different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:22:02.026643Z",
     "start_time": "2024-01-11T05:22:02.020946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with the correct encoding:\n",
      "A line\n",
      "Another line\n",
      "A line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\n",
      "\n",
      "This is with the wrong encoding:\n",
      "A line\n",
      "Another line\n",
      "A line with a few unusual symbols â¡ â â  â¡ â¢ â£ àµ \n"
     ]
    }
   ],
   "source": [
    "with open(example_text_file, encoding='utf-8') as f:\n",
    "    print(\"This is with the correct encoding:\")\n",
    "    print(f.read())\n",
    "\n",
    "with open(example_text_file, encoding='latin-1') as f:\n",
    "    print(\"This is with the wrong encoding:\")\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.667732Z"
    }
   },
   "source": [
    "Notice that with _latin-1_ the unicode characters are mixed up and there are too\n",
    "many of them. You need to keep in mind encoding when obtaining text files.\n",
    "Determining the encoding can sometime involve substantial work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load many text files at once. Lets start by looking at the Shakespeare files in the `data` directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T05:22:02.102048Z",
     "start_time": "2024-01-11T05:22:02.027423Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/Shakespeare/midsummer_nights_dream.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[184], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./data/Shakespeare/midsummer_nights_dream.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      2\u001B[0m     midsummer \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(midsummer[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m700\u001B[39m:])\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:286\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    283\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    284\u001B[0m     )\n\u001B[0;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './data/Shakespeare/midsummer_nights_dream.txt'"
     ]
    }
   ],
   "source": [
    "with open('./data/Shakespeare/midsummer_nights_dream.txt') as f:\n",
    "    midsummer = f.read()\n",
    "print(midsummer[-700:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.668929Z"
    }
   },
   "source": [
    "By the way, depending on your working directory, you might get errors such as: [Errno 2] No such file or directory: '../data/Shakespeare/midsummer_nights_dream.txt.' Don't panic, it's nothing, just check your working directory. \n",
    "\n",
    "Then to load all the files in `./data/Shakespeare` we can use a for loop with `scandir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.041918Z"
    }
   },
   "outputs": [],
   "source": [
    "targetDir = './data/Shakespeare' #Change this to your own directory of texts\n",
    "shakespearText = []\n",
    "shakespearFileName = []\n",
    "\n",
    "for file in (file for file in os.scandir(targetDir) if file.is_file() and not file.name.startswith('.')):\n",
    "    with open(file.path, encoding=\"utf-8\") as f:\n",
    "        shakespearText.append(f.read())\n",
    "    shakespearFileName.append(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.670292Z"
    }
   },
   "source": [
    "Then we can put them all in pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.043858Z"
    }
   },
   "outputs": [],
   "source": [
    "shakespear_df = pandas.DataFrame({'text' : shakespearText}, index = shakespearFileName)\n",
    "shakespear_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.671473Z"
    }
   },
   "source": [
    "Getting your text in a format like this is the first step of most analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF\n",
    "\n",
    "Another common way text will be stored is in a PDF file. First we will download\n",
    "a pdf in Python. To do that lets grab a chapter from\n",
    "_Speech and Language Processing_, chapter 21 is on Information Extraction which\n",
    "seems apt. It is stored as a pdf at [https://web.stanford.edu/~jurafsky/slp3/21.\n",
    "pdf](https://web.stanford.edu/~jurafsky/slp3/21.pdf) although we are downloading\n",
    "from a copy just in case Jurafsky changes their website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.045290Z"
    }
   },
   "outputs": [],
   "source": [
    "#information_extraction_pdf = 'https://github.com/KnowledgeLab/content_analysis/raw/data/21.pdf'\n",
    "\n",
    "infoExtractionRequest = requests.get(information_extraction_pdf, stream=True)\n",
    "print(infoExtractionRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.672558Z"
    }
   },
   "source": [
    "It says `'pdf'`, so thats a good sign. The rest though looks like we are having\n",
    "issues with an encoding. The random characters are not caused by our encoding\n",
    "being wrong, however. They are cause by there not being an encoding for those\n",
    "parts at all. PDFs are nominally binary files, meaning there are sections of\n",
    "binary that are specific to pdf and nothing else so you need something that\n",
    "knows about pdf to read them. To do that we will be using\n",
    "[`PyPDF2`](https://github.com/mstamy2/PyPDF2), a PDF processing library for\n",
    "Python 3.\n",
    "\n",
    "\n",
    "Because PDFs are a very complicated file format pdfminer requires a large amount\n",
    "of boilerplate code to extract text, we have written a function that takes in an\n",
    "open PDF file and returns the text so you don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.046805Z"
    }
   },
   "outputs": [],
   "source": [
    "def readPDF(pdfFile):\n",
    "    #Based on code from http://stackoverflow.com/a/20905381/4955164\n",
    "    #Using utf-8, if there are a bunch of random symbols try changing this\n",
    "    codec = 'utf-8'\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    layoutParams = pdfminer.layout.LAParams()\n",
    "    device = pdfminer.converter.TextConverter(rsrcmgr, retstr, laparams = layoutParams, codec = codec)\n",
    "    #We need a device and an interpreter\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(pdfFile, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    device.close()\n",
    "    returnedString = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return returnedString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.673354Z"
    }
   },
   "source": [
    "First we need to take the response object and convert it into a 'file like'\n",
    "object so that pdfminer can read it. To do this we will use `io`'s `BytesIO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.048515Z"
    }
   },
   "outputs": [],
   "source": [
    "infoExtractionBytes = io.BytesIO(infoExtractionRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.674226Z"
    }
   },
   "source": [
    "Now we can give it to pdfminer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.049835Z"
    }
   },
   "outputs": [],
   "source": [
    "print(readPDF(infoExtractionBytes)[:550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.675024Z"
    }
   },
   "source": [
    "From here we can either look at the full text or fiddle with our PDF reader and\n",
    "get more information about individual blocks of text.\n",
    "\n",
    "## Word Docs\n",
    "\n",
    "The other type of document you are likely to encounter is the `.docx`, these are\n",
    "actually a version of [XML](https://en.wikipedia.org/wiki/Office_Open_XML), just\n",
    "like HTML, and like HTML we will use a specialized parser.\n",
    "\n",
    "For this class we will use [`python-docx`](https://python-\n",
    "docx.readthedocs.io/en/latest/) which provides a nice simple interface for\n",
    "reading `.docx` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.050915Z"
    }
   },
   "outputs": [],
   "source": [
    "#example_docx = 'https://github.com/KnowledgeLab/content_analysis/raw/data/example_doc.docx'\n",
    "\n",
    "r = requests.get(example_docx, stream=True)\n",
    "d = docx.Document(io.BytesIO(r.content))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.675971Z"
    }
   },
   "source": [
    "This procedure uses the `io.BytesIO` class again, since `docx.Document` expects\n",
    "a file. Another way to do it is to save the document to a file and then read it\n",
    "like any other file. If we do this we can either delete the file afterwords, or\n",
    "save it and avoid downloading the following time.\n",
    "\n",
    "This function is useful as a part of many different tasks so it and others like it will be added to the helper package `lucem_illud` so we can use it later without having to retype it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.051894Z"
    }
   },
   "outputs": [],
   "source": [
    "def downloadIfNeeded(targetURL, outputFile, **openkwargs):\n",
    "    if not os.path.isfile(outputFile):\n",
    "        outputDir = os.path.dirname(outputFile)\n",
    "        #This function is a more general os.mkdir()\n",
    "        if len(outputDir) > 0:\n",
    "            os.makedirs(outputDir, exist_ok = True)\n",
    "        r = requests.get(targetURL, stream=True)\n",
    "        #Using a closure like this is generally better than having to\n",
    "        #remember to close the file. There are ways to make this function\n",
    "        #work as a closure too\n",
    "        with open(outputFile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    return open(outputFile, **openkwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.676731Z"
    }
   },
   "source": [
    "This function will download, save and open `outputFile` as `outputFile` or just\n",
    "open it if `outputFile` exists. By default `open()` will open the file as read\n",
    "only text with the local encoding, which may cause issues if its not a text\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.052801Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    d = docx.Document(downloadIfNeeded(example_docx, example_docx_save))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.677517Z"
    }
   },
   "source": [
    "We need to tell `open()` to read in binary mode (`'rb'`), this is why we added\n",
    "`**openkwargs`, this allows us to pass any keyword arguments (kwargs) from\n",
    "`downloadIfNeeded` to `open()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.053652Z"
    }
   },
   "outputs": [],
   "source": [
    "d = docx.Document(downloadIfNeeded(example_docx, example_docx_save, mode = 'rb'))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-10T04:59:16.678301Z"
    }
   },
   "source": [
    "Now we can read the file with `docx.Document` and not have to wait for it to be\n",
    "downloaded every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"white\">Exercise 3</font>\n",
    "<font color=\"white\">Construct cells immediately below this that extract and organize textual content from text, PDF or Word into a pandas dataframe.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_dir = '/Users/caojie/Desktop/MACSS/2024 winter/context analysis/1/txt'\n",
    "text = []\n",
    "file_name = []\n",
    "for file in (file for file in os.scandir(target_dir) if file.is_file() and not file.name.startswith('.')):\n",
    "    with open(file.path, encoding=\"gbk\") as f:\n",
    "        text.append(f.read())\n",
    "    file_name.append(file.name)\n",
    "news_df = pandas.DataFrame({'text' : text}, index = file_name)\n",
    "news_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-11T05:22:02.054541Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"white\">Exercise 4</font>\n",
    "\n",
    "<font color=\"white\">In the two cells immediately following, describe a possible project (e.g., it might end up being your final project, but need not be if you are still searching): **WHAT** you will analyze--the texts you will select and the social game, world and actors you intend to learn about through your analysis (<100 words); **WHY** you will analyze these texts to learn about that context--justify the rationale behind your proposed sample design for this project, based on the readings. What is the social game, social work, or social actors about whom you are seeking to make inferences? What are the virtues of your proposed sample with respect to your research questions? What are its limitations? What are alternatives? What would be a reasonable path to \"scale up\" your sample for further analysis (i.e., high-profile publication)? (<150 words)? [**Note**: your individual or collective project will change over the course of the quarter as new data and/or analysis opportunities arise or if old ones fade away.] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***What?*** \n",
    "I plan to analyze the text data released by local governments in China. The text data includes but is not limited to articles from local governmental newspapers, governmental information disclosure content on the websites of various subordinate departments, and blog posts of local government accounts on Chinese social media platforms. I might focus on corpora about certain topics such as prevention and control of Covid-19 or foreign policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Why?***\n",
    "Nowadays, the Internet has made information communication possible on a historically large scale, which also provides us with another possible perspective to observe governmental behavior. As a seemingly highly centralized regime, the conflict and cooperation between local governments in China has long been an undervalued issue, which is partly caused by the lack of research materials, as these internal conflicts are generally thought to be resolved behind closed doors within the CPC. However, the Internet has changed the way governments communicate with each other as well as with their citizens. Thus, I believe that it would be interesting to explore the inter-governmental relationship more on the basis of a large-scale online dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "\n",
    "Other popular sources for internet data:\n",
    "\n",
    "[reddit](https://www.reddit.com/) - https://praw.readthedocs.io/en/v2.1.21/\n",
    "\n",
    "[twitter](https://twitter.com/) - https://pypi.org/project/python-twitter/\n",
    "\n",
    "[project gutenburg](https://www.gutenberg.org/) - https://github.com/ageitgey/Gutenberg \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
